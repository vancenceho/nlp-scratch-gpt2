{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pmLDoL2a5Fi_"
      },
      "id": "pmLDoL2a5Fi_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/kp-gpt2-nlp\")\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "id": "UUn58poR5Cue"
      },
      "id": "UUn58poR5Cue",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96361c6c",
      "metadata": {
        "id": "96361c6c"
      },
      "outputs": [],
      "source": [
        "%pip install -q -r requirements_colab.txt\n",
        "print(\"Dependencies installed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2903822",
      "metadata": {
        "id": "a2903822"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, dtype\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Optimizer\n",
        "from torch.utils.data import random_split, DataLoader, Dataset\n",
        "from config import PretrainedConfig, GPT2Config\n",
        "from transformers import GPT2Model as OpenAIGPT2Model\n",
        "from transformers import GPT2Tokenizer\n",
        "from utils import *\n",
        "from einops import rearrange\n",
        "from typing import Callable, Iterable, Tuple\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23384a93",
      "metadata": {
        "id": "23384a93"
      },
      "source": [
        "# Task 1: Implement GPT-2\n",
        "In this task, you will:\n",
        "- Load the GPT-2 tokenizer.\n",
        "- Implement the GPT-2 model.\n",
        "- Implement the Adam optimizer.\n",
        "- Conduct a toy pretraining of GPT-2 on the provided small dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e498552",
      "metadata": {
        "id": "6e498552"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eb30c57",
      "metadata": {
        "id": "5eb30c57"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "text = \"Welcome, this is the beginning of default final project!\"\n",
        "input_ids = tokenizer(text)['input_ids']\n",
        "print('input_ids:', input_ids)\n",
        "for token in input_ids:\n",
        "    print('token', tokenizer.decode(token))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bd74b99",
      "metadata": {
        "id": "0bd74b99"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "061db6ef",
      "metadata": {
        "id": "061db6ef"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_attention_heads = config.num_attention_heads\n",
        "    self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "    self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "    # Initialize the linear transformation layers for key, value, query.\n",
        "    self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "    self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "    self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "    # This dropout is applied to normalized attention scores following the original\n",
        "    # implementation of transformer. Although it is a bit unusual, we empirically\n",
        "    # observe that it yields better performance.\n",
        "    self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "  def transform(self, x, linear_layer):\n",
        "    # The corresponding linear_layer of k, v, q are used to project the hidden_state (x).\n",
        "    proj = linear_layer(x)\n",
        "    # Next, we need to produce multiple heads for the proj. This is done by spliting the\n",
        "    # hidden state to self.num_attention_heads, each of size self.attention_head_size.\n",
        "    proj = rearrange(proj, 'b t (h d) -> b t h d', h=self.num_attention_heads)\n",
        "    # By proper transpose, we have proj of size [bs, num_attention_heads, seq_len, attention_head_size].\n",
        "    proj = rearrange(proj, 'b t h d -> b h t d')\n",
        "    return proj\n",
        "\n",
        "  def attention(self, key, query, value, attention_mask):\n",
        "    \"\"\"\n",
        "    TODO-1: Compute scaled dot-product attention for GPT-2.\n",
        "\n",
        "    Implementation hints:\n",
        "    1. Compute raw attention scores using QK^T, and scale them by sqrt(d_k).\n",
        "    2. Apply a causal mask (lower-triangular) to prevent attending to future tokens.\n",
        "    3. Optionally add the external attention_mask (e.g., padding positions).\n",
        "    4. Normalize the scores with softmax to obtain attention probabilities.\n",
        "    5. Apply dropout on the probabilities.\n",
        "    6. Use them to weight the values (V) and obtain the context vectors.\n",
        "    7. Finally, merge all attention heads back into a single hidden representation.\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    # Compute raw attention scores using QK^T, and scale them by sqrt(d_k).\n",
        "    # q, k, v shapes: [B, H, T, d]\n",
        "    raw_scores = torch.matmul(query, key.transpose(-2, -1))  # [B, H, T, T]\n",
        "    d_k = query.size(-1)\n",
        "    scaled_scores = raw_scores / (torch.sqrt(torch.tensor(d_k, dtype=torch.float32)))\n",
        "\n",
        "    # Apply a causal mask over the token dimension (lower triangular over T x T).\n",
        "    Tq = query.size(-2)\n",
        "    Tk = key.size(-2)\n",
        "    causal = torch.tril(torch.ones((Tq, Tk), device=query.device, dtype=torch.bool))  # [Tq, Tk]; 1 for keep, 0 for mask; create a lower triangular mask\n",
        "    scaled_scores = scaled_scores.masked_fill(~causal, torch.finfo(scaled_scores.dtype).min)  # Fill masked positions with -inf\n",
        "\n",
        "    # Optionally add the external attention_mask (e.g., padding positions).\n",
        "    if attention_mask is not None:\n",
        "        # attention_mask is already in logit space: 0 for keep, large negative for mask\n",
        "        scaled_scores = scaled_scores + attention_mask\n",
        "\n",
        "    # Normalize the scores with softmax to obtain attention probabilities.\n",
        "    attention_probs = F.softmax(scaled_scores, dim=-1)\n",
        "\n",
        "    # Apply dropout to the probabilities.\n",
        "    attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "    # Weight the values (V) and obtain the context vectors.\n",
        "    context_vectors = torch.matmul(attention_probs, value)  # [B, H, T, d]\n",
        "\n",
        "    # Merge all attention heads back into a single hidden representation.\n",
        "    context_vectors = rearrange(context_vectors, 'b h t d -> b t (h d)')\n",
        "\n",
        "    return context_vectors\n",
        "\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "  def forward(self, hidden_states, attention_mask):\n",
        "    \"\"\"\n",
        "    hidden_states: [bs, seq_len, hidden_state]\n",
        "    attention_mask: [bs, 1, 1, seq_len]\n",
        "    output: [bs, seq_len, hidden_state]\n",
        "    \"\"\"\n",
        "    # First, we have to generate the key, value, query for each token for multi-head attention\n",
        "    # using self.transform (more details inside the function).\n",
        "    # Size of *_layer is [bs, num_attention_heads, seq_len, attention_head_size].\n",
        "    key_layer = self.transform(hidden_states, self.key)\n",
        "    value_layer = self.transform(hidden_states, self.value)\n",
        "    query_layer = self.transform(hidden_states, self.query)\n",
        "\n",
        "    # Calculate the multi-head attention using the self.attention function.\n",
        "    attn_value = self.attention(key_layer, query_layer, value_layer, attention_mask)\n",
        "    return attn_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc9ad59",
      "metadata": {
        "id": "efc9ad59"
      },
      "outputs": [],
      "source": [
        "class GPT2Layer(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    # Multi-head attention.\n",
        "    self.self_attention = CausalSelfAttention(config)\n",
        "    # Add-norm for multi-head attention.\n",
        "    self.attention_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "    self.attention_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "    self.attention_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "    # Feed forward.\n",
        "    self.interm_dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "    self.interm_af = F.gelu\n",
        "    # Add-norm for feed forward.\n",
        "    self.out_dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "    self.out_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "    self.out_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "  def add(self, input, output, dense_layer, dropout):\n",
        "    \"\"\"\n",
        "    TODO-2: Residual connection with dense projection and dropout.\n",
        "\n",
        "    Implementation hints:\n",
        "    1. Project the 'output' through dense_layer.\n",
        "    2. Apply dropout to prevent overfitting.\n",
        "    3. Add the original 'input' (residual connection) to the processed output.\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    # Apply dense projection to the output.\n",
        "    output = dense_layer(output)\n",
        "\n",
        "    # Apply dropout to prevent overfitting.\n",
        "    output = dropout(output)\n",
        "\n",
        "    # Add original 'input' (residual connection) to the processed output.\n",
        "    output = input + output\n",
        "\n",
        "    return output\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "  def forward(self, hidden_states, attention_mask):\n",
        "    \"\"\"\n",
        "    TODO-3: Forward pass of a GPT-2 layer.\n",
        "\n",
        "    Implementation hints:\n",
        "    ---- Self-Attention Block ----\n",
        "    1. LayerNorm the input for stability using self.attention_layer_norm.\n",
        "    2. Compute multi-head causal self-attention using self.self_attention.\n",
        "    3. Apply residual connection using self.add (dense_layer=self.attention_dense, dropout=self.attention_dropout).\n",
        "\n",
        "    ---- Feed Forward Block ----\n",
        "    4. LayerNorm the hidden_states from attention block using self.out_layer_norm.\n",
        "    5. Pass through a two-layer feed-forward network with activation:\n",
        "       self.interm_dense -> self.interm_af -> self.out_dense\n",
        "    6. Apply residual connection again using self.add (dense_layer=self.out_dense, dropout=self.out_dropout).\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    # ---- Self-Attention Block (Pre-LN) ----\n",
        "    residual = hidden_states\n",
        "    # LayerNorm the input for stability using self.attention_layer_norm.\n",
        "    normed = self.attention_layer_norm(hidden_states)\n",
        "    # Compute multi-head causal self-attention using self.self_attention.\n",
        "    attention_output = self.self_attention(normed, attention_mask)\n",
        "    # Apply residual connection using self.add (dense_layer=self.attention_dense, dropout=self.attention_dropout).\n",
        "    hidden_states = self.add(residual, attention_output, self.attention_dense, self.attention_dropout)\n",
        "\n",
        "    # ---- Feed Forward Block (Pre-LN) ----\n",
        "    residual = hidden_states\n",
        "    # LayerNorm the hidden_states from attention block using self.out_layer_norm.\n",
        "    normed = self.out_layer_norm(hidden_states)\n",
        "    # Pass through a two-layer feed-forward network with activation: self.interm_dense -> self.interm_af -> self.out_dense\n",
        "    ffn_preproj = self.interm_dense(normed)\n",
        "    ffn_preproj = self.interm_af(ffn_preproj)\n",
        "    # Apply residual connection again using self.add (dense_layer=self.out_dense, dropout=self.out_dropout).\n",
        "    hidden_states = self.add(residual, ffn_preproj, self.out_dense, self.out_dropout)\n",
        "\n",
        "    return hidden_states\n",
        "\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70a04146",
      "metadata": {
        "id": "70a04146"
      },
      "outputs": [],
      "source": [
        "class GPTPreTrainedModel(nn.Module):\n",
        "\n",
        "  def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.name_or_path = config.name_or_path\n",
        "\n",
        "  def init_weights(self):\n",
        "    # Initialize weights\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    \"\"\" Initialize the weights \"\"\"\n",
        "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "      # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "      # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "      module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "    elif isinstance(module, nn.LayerNorm):\n",
        "      module.bias.data.zero_()\n",
        "      module.weight.data.fill_(1.0)\n",
        "    if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "      module.bias.data.zero_()\n",
        "\n",
        "  @property\n",
        "  def dtype(self) -> dtype:\n",
        "    return get_parameter_dtype(self)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e68d1cd2",
      "metadata": {
        "id": "e68d1cd2"
      },
      "outputs": [],
      "source": [
        "class GPT2Model(GPTPreTrainedModel):\n",
        "  \"\"\"\n",
        "  The GPT model returns the final embeddings for each token in a sentence.\n",
        "\n",
        "  The model consists of:\n",
        "  1. Embedding layers (used in self.embed).\n",
        "  2. A stack of n GPT layers (used in self.encode).\n",
        "  3. A linear transformation layer for the [CLS] token (used in self.forward, as given).\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.config = config\n",
        "\n",
        "    # Embedding layers.\n",
        "    self.word_embedding = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "    self.pos_embedding = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "    self.embed_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    # Register position_ids (1, len position emb) to buffer because it is a constant.\n",
        "    position_ids = torch.arange(config.max_position_embeddings).unsqueeze(0)\n",
        "    self.register_buffer('position_ids', position_ids)\n",
        "\n",
        "    # GPT-2 layers.\n",
        "    self.gpt_layers = nn.ModuleList([GPT2Layer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    # [CLS] token transformations.\n",
        "    self.pooler_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "    self.pooler_af = nn.Tanh()\n",
        "\n",
        "    # Final layer norm.\n",
        "    self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def embed(self, input_ids):\n",
        "    \"\"\"\n",
        "    TODO-4: Embedding layer of the GPT-2 model.\n",
        "\n",
        "    Implementation hints:\n",
        "    1. Use self.word_embedding to convert input_ids to embeddings.\n",
        "    2. Generate position ids and convert to embeddings using self.pos_embedding.\n",
        "    3. Sum token and position embeddings.\n",
        "    4. Apply self.embed_dropout to the sum.\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    # Use self.word_embedding to convert input_ids to embeddings.\n",
        "    embeddings = self.word_embedding(input_ids)\n",
        "\n",
        "    # Generate position ids and convert to embeddings using self.pos_embedding.\n",
        "    position_ids = self.position_ids[:, :input_ids.shape[1]]\n",
        "    position_embeddings = self.pos_embedding(position_ids)\n",
        "\n",
        "    # Sum token and position embeddings.\n",
        "    embeddings = embeddings + position_embeddings\n",
        "\n",
        "    # Apply self.embed_dropout to the sum.\n",
        "    embeddings = self.embed_dropout(embeddings)\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def encode(self, hidden_states, attention_mask):\n",
        "    \"\"\"\n",
        "    hidden_states: the output from the embedding layer [batch_size, seq_len, hidden_size]\n",
        "    attention_mask: [batch_size, seq_len]\n",
        "    \"\"\"\n",
        "    # Get the extended attention mask for self-attention.\n",
        "    # Returns extended_attention_mask of size [batch_size, 1, 1, seq_len].\n",
        "    # Distinguishes between non-padding tokens (with a value of 0) and padding tokens\n",
        "    # (with a value of a large negative number).\n",
        "    extended_attention_mask: torch.Tensor = get_extended_attention_mask(attention_mask, self.dtype)\n",
        "\n",
        "    # Pass the hidden states through the encoder layers.\n",
        "    for i, layer_module in enumerate(self.gpt_layers):\n",
        "      # Feed the encoding from the last bert_layer to the next.\n",
        "      hidden_states = layer_module(hidden_states, extended_attention_mask)\n",
        "\n",
        "    return hidden_states\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    \"\"\"\n",
        "    input_ids: [batch_size, seq_len], seq_len is the max length of the batch\n",
        "    attention_mask: same size as input_ids, 1 represents non-padding tokens, 0 represents padding tokens\n",
        "    \"\"\"\n",
        "    # Get the embedding for each input token.\n",
        "    embedding_output = self.embed(input_ids=input_ids)\n",
        "\n",
        "    # Feed to a transformer (a stack of GPTLayers).\n",
        "    sequence_output = self.encode(embedding_output, attention_mask=attention_mask)\n",
        "    sequence_output = self.final_layer_norm(sequence_output)\n",
        "\n",
        "    # Get the hidden state of the final token.\n",
        "    last_non_pad_idx = attention_mask.sum(dim=1) - 1  # Subtract 1 to get last index\n",
        "    last_token = sequence_output[torch.arange(sequence_output.shape[0]), last_non_pad_idx]\n",
        "\n",
        "    return {'last_hidden_state': sequence_output, 'last_token': last_token}\n",
        "\n",
        "  def hidden_state_to_token(self, hidden_state):\n",
        "    \"\"\"\n",
        "    TODO-5: Convert hidden states back to token logits.\n",
        "\n",
        "    Implementation hints:\n",
        "    - GPT-2 uses weight tying with the input word embeddings.\n",
        "    - The logits are the dot product between output hidden states and the word embedding weights: hidden_state(s) * E^T\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    # Compute the dot product between the hidden states and the word embedding weights: hidden_state(s) * E^T\n",
        "    logits = torch.matmul(hidden_state, self.word_embedding.weight.T)\n",
        "\n",
        "    return logits\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "  @classmethod\n",
        "  def from_pretrained(cls, model='gpt2', d=768, l=12, num_heads=12):\n",
        "    gpt_model = OpenAIGPT2Model.from_pretrained(model).eval()\n",
        "    our_model = GPT2Model(GPT2Config(hidden_size=d, num_hidden_layers=l,num_attention_heads=num_heads,\n",
        "                                     intermediate_size=d*3)).eval()\n",
        "\n",
        "    # Load word and positional embeddings.\n",
        "    our_model.word_embedding.load_state_dict(gpt_model.wte.state_dict())\n",
        "    our_model.pos_embedding.load_state_dict(gpt_model.wpe.state_dict())\n",
        "\n",
        "    for i in range(l):\n",
        "      l = our_model.gpt_layers[i]\n",
        "      # Remap the Q,K,V weights from a conv1d to 3 linear projections\n",
        "      l.self_attention.query.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.weight'][:, :d].T\n",
        "      l.self_attention.query.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.bias'][:d]\n",
        "      l.self_attention.key.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.weight'][:, d:d*2].T\n",
        "      l.self_attention.key.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.bias'][d:d*2]\n",
        "      l.self_attention.value.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.weight'][:, d*2:].T\n",
        "      l.self_attention.value.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.bias'][d*2:]\n",
        "\n",
        "      # Remap final dense layer in MHA.\n",
        "      l.attention_dense.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_proj.weight'].T\n",
        "      l.attention_dense.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_proj.bias']\n",
        "\n",
        "      # Remap attention layer norm.\n",
        "      l.attention_layer_norm.weight.data = gpt_model.state_dict()[f'h.{i}.ln_1.weight']\n",
        "      l.attention_layer_norm.bias.data = gpt_model.state_dict()[f'h.{i}.ln_1.bias']\n",
        "\n",
        "      # Remap post-attention MLP layers.\n",
        "      l.interm_dense.weight.data = gpt_model.state_dict()[f'h.{i}.mlp.c_fc.weight'].T\n",
        "      l.interm_dense.bias.data = gpt_model.state_dict()[f'h.{i}.mlp.c_fc.bias']\n",
        "      l.out_dense.weight.data = gpt_model.state_dict()[f'h.{i}.mlp.c_proj.weight'].T\n",
        "      l.out_dense.bias.data = gpt_model.state_dict()[f'h.{i}.mlp.c_proj.bias']\n",
        "\n",
        "      # Remap second layer norm weights.\n",
        "      l.out_layer_norm.weight.data = gpt_model.state_dict()[f'h.{i}.ln_2.weight']\n",
        "      l.out_layer_norm.bias.data = gpt_model.state_dict()[f'h.{i}.ln_2.bias']\n",
        "\n",
        "    # Remap the final layer norm values.\n",
        "    our_model.final_layer_norm.weight.data = gpt_model.state_dict()['ln_f.weight']\n",
        "    our_model.final_layer_norm.bias.data = gpt_model.state_dict()['ln_f.bias']\n",
        "\n",
        "    return our_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c95668",
      "metadata": {
        "id": "97c95668"
      },
      "outputs": [],
      "source": [
        "# Sanity check: compare with Huggingface GPT2 implementation\n",
        "def test_gpt2(model_size='gpt2'):\n",
        "  sent_ids = torch.tensor([[101, 7592, 2088, 102, 0, 0, 0, 0],\n",
        "                           [101, 7592, 15756, 2897, 2005, 17953, 2361, 102]])\n",
        "  att_mask = torch.tensor([[1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1]])\n",
        "\n",
        "  # Load both the OpenAI and your own model.\n",
        "  openai_model = OpenAIGPT2Model.from_pretrained(model_size)\n",
        "  gpt = GPT2Model.from_pretrained(model=model_size, **model_size_to_params(model_size))\n",
        "\n",
        "  outputs = gpt(sent_ids, att_mask)\n",
        "  openai_outputs = openai_model(input_ids=sent_ids, attention_mask=att_mask, output_hidden_states=True).hidden_states[-1]\n",
        "\n",
        "  att_mask = att_mask.unsqueeze(-1)\n",
        "  outputs['last_hidden_state'] = outputs['last_hidden_state'] * att_mask\n",
        "  openai_outputs *= att_mask\n",
        "\n",
        "  assert torch.allclose(outputs['last_hidden_state'], openai_outputs, atol=1e-1, rtol=1e-2)\n",
        "\n",
        "  print(\"Your GPT2 implementation is correct!\")\n",
        "\n",
        "test_gpt2('gpt2')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3a449a7",
      "metadata": {
        "id": "c3a449a7"
      },
      "source": [
        "## Adam Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35494be8",
      "metadata": {
        "id": "35494be8"
      },
      "outputs": [],
      "source": [
        "class AdamW(Optimizer):\n",
        "    def __init__(\n",
        "            self,\n",
        "            params: Iterable[torch.nn.parameter.Parameter],\n",
        "            lr: float = 1e-3,\n",
        "            betas: Tuple[float, float] = (0.9, 0.999),\n",
        "            eps: float = 1e-6,\n",
        "            weight_decay: float = 0.0,\n",
        "            correct_bias: bool = True,\n",
        "    ):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(eps))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, correct_bias=correct_bias)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure: Callable = None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
        "\n",
        "                # State should be stored in this dictionary.\n",
        "                state = self.state[p]\n",
        "\n",
        "                # Access hyperparameters from the `group` dictionary.\n",
        "                lr = group[\"lr\"]\n",
        "                eps = group[\"eps\"]\n",
        "                weight_decay = group[\"weight_decay\"]\n",
        "                correct_bias = group[\"correct_bias\"]\n",
        "                beta1, beta2 = group[\"betas\"]\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state[\"step\"] = 0\n",
        "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
        "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
        "\n",
        "                state[\"step\"] += 1\n",
        "                t = state[\"step\"]\n",
        "\n",
        "                \"\"\"\n",
        "                TODO-6: Implement the AdamW parameter update for this step.\n",
        "\n",
        "                Implementation hints:\n",
        "                1. Update biased first moment estimate:\n",
        "                    m_t = beta1 * m_{t-1} + (1 - beta1) * grad\n",
        "                2. Update biased second raw moment estimate:\n",
        "                    v_t = beta2 * v_{t-1} + (1 - beta2) * grad^2\n",
        "                3. Apply bias correction if correct_bias=True:\n",
        "                    m_hat = m_t / (1 - beta1^t)\n",
        "                    v_hat = v_t / (1 - beta2^t)\n",
        "                4. Compute step size:\n",
        "                    step_size = lr (or lr / (1 - beta1^t) if bias correction)\n",
        "                5. Update parameters:\n",
        "                    p = p - step_size * m_hat / (sqrt(v_hat) + eps)\n",
        "                6. Apply decoupled weight decay after the parameter update (if weight_decay > 0):\n",
        "                    p = p - lr * weight_decay * p\n",
        "                Reference:\n",
        "                Algorithm 1 in \"Adam: A Method for Stochastic Optimization\"\n",
        "                https://arxiv.org/abs/1412.6980\n",
        "                \"\"\"\n",
        "                ### YOUR CODE HERE\n",
        "\n",
        "                # Update biased first moment estimate: m_t = beta1 * m_{t-1} + (1 - beta1) * grad\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "                # Update biased second raw moment estimate: v_t = beta2 * v_{t-1} + (1 - beta2) * grad^2\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "\n",
        "                # Apply bias correction if correct_bias=True:\n",
        "                if correct_bias:\n",
        "                    # m_hat = m_t / (1 - beta1^t)\n",
        "                    bias_correction1 = 1 - beta1 ** t\n",
        "                    # v_hat = v_t / (1 - beta2^t)\n",
        "                    bias_correction2 = 1 - beta2 ** t\n",
        "                    # Compute step size: step_size = lr / (1 - beta1^t) if bias correction\n",
        "                    step_size = lr * (bias_correction2 ** 0.5) / bias_correction1\n",
        "                else:\n",
        "                    # Compute step size: step_size = lr\n",
        "                    step_size = lr\n",
        "\n",
        "                # Update parameters: p = p - step_size * m_hat / (sqrt(v_hat) + eps)\n",
        "                # Denominator = sqrt(v_hat) + eps\n",
        "                denominator = exp_avg_sq.sqrt().add_(eps)\n",
        "                # Parameter update: p = p - step_size * m_hat / (sqrt(v_hat) + eps)\n",
        "                p.data.addcdiv_(exp_avg, denominator, value=-step_size)\n",
        "\n",
        "                # Apply decoupled weight decay after the parameter update (if weight_decay > 0):\n",
        "                if weight_decay > 0:\n",
        "                    p.data.add_(p.data, alpha=-lr * weight_decay)\n",
        "        return loss\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "544e2a8c",
      "metadata": {
        "id": "544e2a8c"
      },
      "outputs": [],
      "source": [
        "# Sanity check for AdamW optimizer\n",
        "def test_optimizer(opt_class) -> torch.Tensor:\n",
        "    rng = np.random.default_rng(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    model = torch.nn.Linear(3, 2, bias=False)\n",
        "    opt = opt_class(\n",
        "        model.parameters(),\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-4,\n",
        "        correct_bias=True,\n",
        "    )\n",
        "    for i in range(1000):\n",
        "        opt.zero_grad()\n",
        "        x = torch.FloatTensor(rng.uniform(size=[model.in_features]))\n",
        "        y_hat = model(x)\n",
        "        y = torch.Tensor([x[0] + x[1], -x[2]])\n",
        "        loss = ((y - y_hat) ** 2).sum()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    return model.weight.detach()\n",
        "\n",
        "SEED = 0\n",
        "ref = torch.tensor(np.load(\"optimizer_test.npy\"))\n",
        "actual = test_optimizer(AdamW)\n",
        "print(ref)\n",
        "print(actual)\n",
        "assert torch.allclose(ref, actual, atol=1e-6, rtol=1e-4)\n",
        "print(\"Optimizer test passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bde426b",
      "metadata": {
        "id": "8bde426b"
      },
      "source": [
        "## Toy GPT-2 Pretraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f03fffce",
      "metadata": {
        "id": "f03fffce"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for preparing text data for language model training.\n",
        "\n",
        "    Each line in the input text file is treated as a separate training example.\n",
        "    The dataset uses a tokenizer to convert text into input IDs and attention masks,\n",
        "    with optional truncation and padding to a fixed maximum sequence length.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the text file. Each line is a separate sample.\n",
        "        tokenizer (PreTrainedTokenizer): Tokenizer to convert text to token IDs.\n",
        "        max_len (int): Maximum sequence length; sequences longer than this are truncated,\n",
        "                       shorter sequences are padded.\n",
        "\n",
        "    Returns per item:\n",
        "        input_ids (torch.Tensor): Token IDs of shape [max_len].\n",
        "        attention_mask (torch.Tensor): Attention mask of shape [max_len], 1 for real tokens, 0 for padding.\n",
        "    \"\"\"\n",
        "    def __init__(self, filepath, tokenizer, max_len):\n",
        "        with open(filepath, 'r') as f:\n",
        "            self.texts = f.read().splitlines()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = enc['input_ids'].squeeze(0)\n",
        "        attention_mask = enc['attention_mask'].squeeze(0)\n",
        "        return input_ids, attention_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34229c7d",
      "metadata": {
        "id": "34229c7d"
      },
      "outputs": [],
      "source": [
        "# Hyperparamter of toy gpt2 pretraining\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 3\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1e-4\n",
        "CORRECT_BIAS = True\n",
        "HIDDEN_SIZE = 128 # 768 for gpt2\n",
        "NUM_HIDDEN_LAYERS = 2 # 12 for gpt2\n",
        "NUM_ATTENTION_HEADS = 4 # 12 for gpt2\n",
        "MAX_SEQ_LEN = 128 # 1024 for gpt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ad3e6bc",
      "metadata": {
        "id": "9ad3e6bc"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Ensure padding has a pad token\n",
        "\n",
        "model_config = GPT2Config(\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_hidden_layers=NUM_HIDDEN_LAYERS,\n",
        "    num_attention_heads=NUM_ATTENTION_HEADS,\n",
        "    intermediate_size=HIDDEN_SIZE*3,\n",
        ")\n",
        "\n",
        "toy_gpt2_model = GPT2Model(model_config).to(DEVICE)\n",
        "\n",
        "VOCAB_SIZE = model_config.vocab_size\n",
        "\n",
        "dataset = TextDataset('pretrain.txt', tokenizer, MAX_SEQ_LEN)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "optimizer = AdamW(toy_gpt2_model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, correct_bias=CORRECT_BIAS)\n",
        "\n",
        "\n",
        "global_train_losses = []\n",
        "\n",
        "total_train_loss = 0.0\n",
        "total_train_steps = 0\n",
        "\n",
        "\n",
        "print_interval = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    toy_gpt2_model.train()\n",
        "    for batch_idx, (input_ids, attention_mask) in enumerate(dataloader):\n",
        "        input_ids = input_ids.to(DEVICE)\n",
        "        attention_mask = attention_mask.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hidden_states = toy_gpt2_model(input_ids, attention_mask)['last_hidden_state']  # [B, seq_len, hidden]\n",
        "\n",
        "        \"\"\"\n",
        "        TODO-7: Compute next-token loss from hidden states and update model parameters.\n",
        "\n",
        "        Implementation hints:\n",
        "        1. Convert hidden states to logits over the vocabulary using model.hidden_state_to_token.\n",
        "        2. Shift logits and labels for next-token prediction to align each prediction with the correct next token.\n",
        "        3. Compute the cross-entropy loss.\n",
        "        4. Backpropagate and update parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "        # Convert hidden states to logits over the vocabulary using model.hidden_state_to_token.\n",
        "        logits = toy_gpt2_model.hidden_state_to_token(hidden_states)\n",
        "        # Shift logits and labels for next-token prediction to align each prediction with the correct next token.\n",
        "        logits = logits[:, :-1, :]\n",
        "        labels = input_ids[:, 1:]\n",
        "        # Compute the cross-entropy loss.\n",
        "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), labels.reshape(-1))\n",
        "        # Backpropagate and update parameters.\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # raise NotImplementedError\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        total_train_steps += 1\n",
        "        global_train_avg_loss = total_train_loss / total_train_steps\n",
        "        global_train_losses.append(global_train_avg_loss)\n",
        "\n",
        "        if batch_idx % print_interval == 0:\n",
        "            print(f\"Train | Epoch {epoch} | Batch {batch_idx} | Global Avg Train Loss: {global_train_avg_loss:.4f}\")\n",
        "\n",
        "    print(f\"Epoch {epoch} finished | Global Avg Train Loss: {global_train_avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d1680f9",
      "metadata": {
        "id": "2d1680f9"
      },
      "outputs": [],
      "source": [
        "# Sanity check: decreasing trend of global average training loss\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(global_train_losses, label=\"Global Avg Train Loss\", color='blue')\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylabel(\"Global Cumulative Avg Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e1a3459",
      "metadata": {
        "id": "5e1a3459"
      },
      "source": [
        "# Task 2: English NLI with GPT-2\n",
        "In this task, you will:\n",
        "- Load a pretrained GPT-2 model with official weights and perform a dummy text generation.\n",
        "- Load an English Natural Language Inference (NLI) dataset.\n",
        "- Fine-tune the loaded model and evaluate its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b249d1f0",
      "metadata": {
        "id": "b249d1f0"
      },
      "source": [
        "## Model Loading & Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc299457",
      "metadata": {
        "id": "fc299457"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate_gpt2(model, tokenizer, input_ids, max_gen_length=50, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Generate text from a GPT-2 model given a single input sequence (greedy decoding).\n",
        "\n",
        "    Note:\n",
        "        - Currently only supports batch_size=1 (single input sequence).\n",
        "        - Using greedy decoding, so each run with the same input produces the same output.\n",
        "        - Other sampling-based decoding methods (e.g., top-k, top-p, temperature) can introduce randomness and yield different outputs each run.\n",
        "\n",
        "    Args:\n",
        "        model: GPT-2 model (pretrained or fine-tuned)\n",
        "        tokenizer: GPT-2 tokenizer\n",
        "        input_ids: torch.LongTensor of shape [1, seq_len], input token IDs\n",
        "        max_gen_length: int, maximum number of tokens to generate\n",
        "        device: str, \"cuda\" or \"cpu\"\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    input_ids = input_ids.to(device)  # move input to device\n",
        "    output_ids = input_ids.clone()\n",
        "\n",
        "    \"\"\"\n",
        "    TODO-8: Greedy next-token generation loop\n",
        "\n",
        "    Implementation hints:\n",
        "    Repeat the below steps up to max_gen_length:\n",
        "    1. Construct an attention mask based on current output_ids (non-pad tokens).\n",
        "    2. Pass output_ids and attention_mask through the model to get hidden states.\n",
        "    3. Convert the last hidden state to logits over the vocabulary using model.hidden_state_to_token.\n",
        "    4. Select the next token using greedy decoding (argmax over logits).\n",
        "    5. Append the next token to output_ids.\n",
        "    6. Stop the loop early if the EOS token is generated.\n",
        "\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    # Store initial length to track how many NEW tokens to generate\n",
        "    initial_length = len(output_ids[0])\n",
        "    while len(output_ids[0]) < initial_length + max_gen_length:\n",
        "\n",
        "        # Construct an attention mask based on current output_ids (non-pad tokens).\n",
        "        attention_mask = torch.ones_like(output_ids)\n",
        "\n",
        "        # Pass output_ids and attention_mask through the model to get hidden states.\n",
        "        hidden_states = model(output_ids, attention_mask)['last_hidden_state']\n",
        "\n",
        "        # Convert the last hidden state to logits over the vocabulary using model.hidden_state_to_token.\n",
        "        logits = model.hidden_state_to_token(hidden_states)\n",
        "\n",
        "        # Select the next token using greedy decoding (argmax over logits).\n",
        "        last_token = logits[:, -1, :]   # [batch_size, vocab_size]\n",
        "        next_token = torch.argmax(last_token, dim=1)\n",
        "\n",
        "        # Append the next token to output_ids.\n",
        "        next_token = next_token.unsqueeze(1)    # add sequence dimension\n",
        "        output_ids = torch.cat([output_ids, next_token], dim=1)\n",
        "\n",
        "        # Stop the loop early if the EOS token is generated.\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    # raise NotImplementedError\n",
        "\n",
        "    # Decode generated tokens to string\n",
        "    ids = output_ids[0]\n",
        "    text = tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e7cc58d",
      "metadata": {
        "id": "3e7cc58d"
      },
      "outputs": [],
      "source": [
        "# Load a pretrained GPT-2 model with official weights\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2Model.from_pretrained(\"gpt2\").to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30d2e14d",
      "metadata": {
        "id": "30d2e14d"
      },
      "outputs": [],
      "source": [
        "# Dummy text generation using the pretrained GPT-2 model with official weights\n",
        "dummy_texts = \"Singapore University of Technology and Design (SUTD) is\"\n",
        "input_ids = tokenizer(dummy_texts, return_tensors=\"pt\", padding=True)['input_ids']\n",
        "generated_texts = generate_gpt2(model, tokenizer, input_ids, max_gen_length=50, device=DEVICE)\n",
        "print(generated_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0f40935",
      "metadata": {
        "id": "b0f40935"
      },
      "outputs": [],
      "source": [
        "# Dummy text generation using the toy GPT-2 model trained in Task 1\n",
        "generated_texts = generate_gpt2(toy_gpt2_model, tokenizer, input_ids, max_gen_length=50, device=DEVICE)\n",
        "print(generated_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b611373",
      "metadata": {
        "id": "7b611373"
      },
      "source": [
        "## Load NLI Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19f25161",
      "metadata": {
        "id": "19f25161"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(preds, labels):\n",
        "    correct = sum(p.lower().strip() == l.lower().strip() for p, l in zip(preds, labels))\n",
        "    return correct / len(labels)\n",
        "\n",
        "def evaluate_gpt2_xnli(model, tokenizer, dataloader, max_gen_length=10, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for item in tqdm(dataloader, desc=\"Generating\"):\n",
        "            input_ids = item['input_ids']\n",
        "            gen_text = generate_gpt2(model, tokenizer, input_ids, max_gen_length=max_gen_length, device=device)\n",
        "            pred_label = gen_text.split(\"Label:\")[-1].strip()\n",
        "            all_preds.append(pred_label)\n",
        "            all_labels.extend(item['label_strs'])\n",
        "    acc = compute_accuracy(all_preds, all_labels)\n",
        "    print(f\"Evaluation accuracy: {acc*100:.2f}%\")\n",
        "    return acc, all_preds, all_labels\n",
        "\n",
        "class XNLIDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for XNLI (Cross-lingual Natural Language Inference) task.\n",
        "\n",
        "    Supports train, dev, and test splits in a specific language,\n",
        "    tokenizes text inputs for GPT-style models, and optionally subsamples the dataset.\n",
        "\n",
        "    Attributes:\n",
        "        split (str): Dataset split, one of 'train', 'dev', 'test'.\n",
        "        lang (str): Language code (e.g., 'en', 'zh').\n",
        "        tokenizer: A HuggingFace tokenizer to convert text to input IDs.\n",
        "        max_length (int): Maximum sequence length for tokenization.\n",
        "        LABEL2ID (dict): Mapping from textual labels to integer IDs.\n",
        "        ID2LABEL (dict): Reverse mapping from integer IDs to textual labels.\n",
        "        data (pd.DataFrame): The loaded and preprocessed dataset.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        split=\"train\",\n",
        "        lang=\"en\",\n",
        "        train_path_template=\"XNLI-MT-1.0/multinli/multinli.train.{lang}.tsv\",\n",
        "        test_path=\"XNLI-1.0/xnli.test.tsv\",\n",
        "        dev_path=\"XNLI-1.0/xnli.dev.tsv\",\n",
        "        tokenizer=None,\n",
        "        max_length=1024,\n",
        "        subset = 1.0  # 0~1\n",
        "    ):\n",
        "        self.split = split\n",
        "        self.lang = lang\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.LABEL2ID = {\"entailment\": 0, \"contradictory\": 1, \"neutral\": 2}\n",
        "        self.ID2LABEL = {v: k for k, v in self.LABEL2ID.items()}\n",
        "\n",
        "        if split == \"train\":\n",
        "            path = train_path_template.format(lang=lang)\n",
        "            df = self.read_xnli_tsv(path, split)\n",
        "            df = df.dropna(subset=['premise','hypo','label'])\n",
        "        elif split in [\"dev\", \"test\"]:\n",
        "            path = test_path if split==\"test\" else dev_path\n",
        "            df = self.read_xnli_tsv(path, split)\n",
        "            df = df[df['language']==lang].copy()\n",
        "            keep_cols = ['sentence1', 'sentence2', 'gold_label']\n",
        "            df = df[keep_cols].dropna()\n",
        "            df.rename(columns={'sentence1':'premise','sentence2':'hypo','gold_label':'label'}, inplace=True)\n",
        "            df['label'] = df['label'].replace({'contradiction': 'contradictory'})\n",
        "        else:\n",
        "            raise ValueError(\"split must be one of ['train','dev','test']\")\n",
        "\n",
        "        original_num = len(df)\n",
        "        if subset < 1.0:\n",
        "            n = max(1, int(len(df) * subset))\n",
        "            df = df.iloc[:n].reset_index(drop=True)\n",
        "        subset_num = len(df)\n",
        "\n",
        "        self.data = df.reset_index(drop=True)\n",
        "        print(f\"Dataset initialized: split='{split}', lang='{lang}', total={original_num}, subset={subset}, subset_count={subset_num}\")\n",
        "\n",
        "    def read_xnli_tsv(self, path, split):\n",
        "        \"\"\"\n",
        "        Read an XNLI TSV file and return it as a pandas DataFrame.\n",
        "\n",
        "        Args:\n",
        "            path (str): Path to the TSV file.\n",
        "            split (str): One of \"train\", \"dev\", \"test\" indicating the dataset split.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: The dataset as a DataFrame with appropriate columns.\n",
        "        \"\"\"\n",
        "        if split == \"train\":\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                lines = f.read().splitlines()\n",
        "            header = lines[0].split(\"\\t\")\n",
        "            data = []\n",
        "            for i, line in enumerate(lines[1:], start=2):\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts) == len(header):\n",
        "                    data.append(parts)\n",
        "                else:\n",
        "                    print(f\"skip row {i}: {len(parts)} cols  {parts[:2]}\")\n",
        "        else:\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                reader = csv.reader(f, delimiter=\"\\t\")\n",
        "                rows = list(reader)\n",
        "            header = rows[0]\n",
        "            expected_cols = len(header)\n",
        "            data = []\n",
        "            for i, row in enumerate(rows[1:], start=2):\n",
        "                if len(row) == expected_cols:\n",
        "                    data.append(row)\n",
        "                else:\n",
        "                    print(f\"skip row {i}: {len(row)} cols  {row[:2]}\")\n",
        "        return pd.DataFrame(data, columns=header)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of examples in the dataset.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieve a single example by index and tokenize it.\n",
        "\n",
        "        For training split:\n",
        "            - Constructs the input as \"Premise: ... Hypothesis: ... Label: ...\"\n",
        "            - Tokenizes the full input.\n",
        "            - Masks the prefix tokens in the labels with -100 for GPT loss computation.\n",
        "\n",
        "        For dev/test split:\n",
        "            - Constructs the input without label as \"Premise: ... Hypothesis: ... Label:\"\n",
        "\n",
        "        Returns:\n",
        "            dict: Contains 'input_ids', 'attention_mask', 'labels' (train only), 'label_str'\n",
        "        \"\"\"\n",
        "        row = self.data.iloc[idx]\n",
        "        premise = row['premise']\n",
        "        hypo = row['hypo']\n",
        "        label = row['label']\n",
        "        if self.lang == 'zh': # de-tokenize for Chinese\n",
        "            premise = premise.replace(\" \", \"\")\n",
        "            hypo = hypo.replace(\" \", \"\")\n",
        "\n",
        "        if self.split == \"train\":\n",
        "            prefix = f\"Premise: {premise}\\nHypothesis: {hypo}\\nLabel:\"\n",
        "            full_text = prefix + str(self.LABEL2ID[label])\n",
        "            tokenized = self.tokenizer(\n",
        "                full_text,\n",
        "                truncation=True,\n",
        "                max_length=self.max_length,\n",
        "                padding=False,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            tokenized = {k: v.squeeze(0) for k, v in tokenized.items()}\n",
        "\n",
        "            prefix_ids = self.tokenizer(prefix).input_ids\n",
        "            labels_ids = tokenized['input_ids'].clone()\n",
        "            labels_ids[:len(prefix_ids)] = -100 # Masks the prefix tokens in the labels with -100 for GPT loss computation.\n",
        "            tokenized['labels'] = labels_ids\n",
        "            tokenized['label_str'] = str(self.LABEL2ID[label])\n",
        "            return tokenized\n",
        "        else:\n",
        "            text = f\"Premise: {premise}\\nHypothesis: {hypo}\\nLabel:\"\n",
        "            tokenized = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                max_length=self.max_length,\n",
        "                padding=False,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            tokenized = {k: v.squeeze(0) for k, v in tokenized.items()}\n",
        "            tokenized['label_str'] = str(self.LABEL2ID[label])\n",
        "            return tokenized\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        \"\"\"\n",
        "        Collate a batch of examples into padded tensors.\n",
        "\n",
        "        Pads 'input_ids' and 'attention_mask' to the max length in the batch.\n",
        "        Pads 'labels' with -100 if present.\n",
        "        Collects 'label_str' for reference.\n",
        "\n",
        "        Returns:\n",
        "            dict: Padded tensors and label strings for the batch.\n",
        "        \"\"\"\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            [b['input_ids'] for b in batch],\n",
        "            batch_first=True,\n",
        "            padding_value=0\n",
        "        )\n",
        "        attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
        "            [b['attention_mask'] for b in batch],\n",
        "            batch_first=True,\n",
        "            padding_value=0\n",
        "        )\n",
        "\n",
        "        if 'labels' in batch[0]:\n",
        "            labels = torch.nn.utils.rnn.pad_sequence(\n",
        "                [b['labels'] for b in batch],\n",
        "                batch_first=True,\n",
        "                padding_value=-100\n",
        "            )\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        label_strs = [b['label_str'] for b in batch]\n",
        "\n",
        "        out = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label_strs\": label_strs}\n",
        "        if labels is not None:\n",
        "            out[\"labels\"] = labels\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fa19d5f",
      "metadata": {
        "id": "0fa19d5f"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Load NLI datasets for fine-tuning and evaluation.\n",
        "For debugging on a CPU, you can set SUBSET to a float in (0,1) to load only a fraction of the data.\n",
        "Final training and evaluation should use the full dataset (SUBSET=1).\n",
        "\"\"\"\n",
        "\n",
        "TRAIN_SUBSET = 1\n",
        "DEV_SUBSET = 1\n",
        "TEST_SUBSET = 1\n",
        "\n",
        "train_dataset = XNLIDataset(\n",
        "    split=\"train\",\n",
        "    lang=\"en\",\n",
        "    tokenizer=tokenizer,\n",
        "    subset=TRAIN_SUBSET\n",
        ")\n",
        "\n",
        "dev_dataset = XNLIDataset(\n",
        "    split=\"dev\",\n",
        "    lang=\"en\",\n",
        "    tokenizer=tokenizer,\n",
        "    subset=DEV_SUBSET\n",
        ")\n",
        "\n",
        "test_dataset = XNLIDataset(\n",
        "    split=\"test\",\n",
        "    lang=\"en\",\n",
        "    tokenizer=tokenizer,\n",
        "    subset=TEST_SUBSET\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3196cd4",
      "metadata": {
        "id": "f3196cd4"
      },
      "source": [
        "## Fine-tune GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aaf7bfd",
      "metadata": {
        "id": "5aaf7bfd"
      },
      "outputs": [],
      "source": [
        "# Hyperparamter of gpt2 fine-tuning\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 4\n",
        "LR = 5e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "CORRECT_BIAS = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e8142b5",
      "metadata": {
        "id": "5e8142b5"
      },
      "outputs": [],
      "source": [
        "# Create DataLoaders for training and validation datasets\n",
        "train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,collate_fn=XNLIDataset.collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset,shuffle=False,collate_fn=XNLIDataset.collate_fn)\n",
        "\n",
        "VOCAB_SIZE = tokenizer.vocab_size\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, correct_bias=CORRECT_BIAS)\n",
        "# Track training progress\n",
        "global_train_losses = []\n",
        "total_train_loss = 0.0\n",
        "total_train_steps = 0\n",
        "print_interval = 10\n",
        "\n",
        "# Track best dev accuracy for model saving\n",
        "# This only works for epoch > 1\n",
        "best_dev_acc = 0.0\n",
        "SAVE_DIR = \"best_model\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    model.train()\n",
        "    # Iterate over batches\n",
        "    loop = tqdm(train_loader, desc=\"Training\")\n",
        "    for batch in loop:\n",
        "        input_ids = batch[\"input_ids\"].to(DEVICE)        # [B, seq_len]\n",
        "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "        labels = batch.get(\"labels\").to(DEVICE)                    # [B, seq_len]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        hidden_states = model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']  # [B, seq_len, hidden]\n",
        "\n",
        "        \"\"\"\n",
        "        TODO-9: Compute next-token loss from hidden states and update model parameters.\n",
        "\n",
        "        Implementation hints:\n",
        "        1. Convert hidden states to logits over the vocabulary using model.hidden_state_to_token.\n",
        "        2. Shift logits and labels for next-token prediction to align each prediction with the correct next token.\n",
        "        3. Compute the cross-entropy loss, making sure positions with label=-100 are ignored.\n",
        "        4. Backpropagate and update model parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "        # Convert hidden states to logits over the vocabulary using model.hidden_state_to_token.\n",
        "        logits = model.hidden_state_to_token(hidden_states)\n",
        "\n",
        "        # Shift logits and labels for next-token prediction to align each prediction with the correct next token.\n",
        "        logits = logits[:, :-1, :]\n",
        "        labels = labels[:, 1:]\n",
        "\n",
        "        # Compute the cross-entropy loss, making sure positions with label=-100 are ignored.\n",
        "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), labels.reshape(-1), ignore_index=-100)\n",
        "\n",
        "        # Backpropagate and update the model parameters.\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # raise NotImplementedError\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        total_train_steps += 1\n",
        "        global_train_avg_loss = total_train_loss / total_train_steps\n",
        "        global_train_losses.append(global_train_avg_loss)\n",
        "\n",
        "        loop.set_postfix({'avg_loss': f\"{global_train_avg_loss:.4f}\"})\n",
        "\n",
        "    print(f\"Epoch {epoch+1} finished | Global Avg Loss: {global_train_avg_loss:.4f}\")\n",
        "\n",
        "    acc, all_preds, all_labels = evaluate_gpt2_xnli(model, tokenizer, dev_loader, max_gen_length=1, device=DEVICE)\n",
        "\n",
        "\n",
        "    if acc > best_dev_acc:\n",
        "        best_dev_acc = acc\n",
        "        torch.save(model.state_dict(), f\"{SAVE_DIR}/model.pt\")\n",
        "        print(f\"New best model saved at {SAVE_DIR}/model.pt with dev accuracy {best_dev_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Diagnostic: Check what the model is actually generating\n",
        "# Run this after training to debug 0% accuracy issues\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DIAGNOSTIC: Checking model predictions\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get a few examples from dev set\n",
        "model.eval()\n",
        "sample_batch = next(iter(dev_loader))\n",
        "input_ids_sample = sample_batch['input_ids'][:3].to(DEVICE)\n",
        "labels_sample = sample_batch['label_strs'][:3]\n",
        "\n",
        "print(f\"\\nExpected labels: {labels_sample}\")\n",
        "print(\"\\nGenerated outputs:\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (input_id, true_label) in enumerate(zip(input_ids_sample, labels_sample)):\n",
        "        input_id = input_id.unsqueeze(0)  # Add batch dimension\n",
        "        gen_text = generate_gpt2(model, tokenizer, input_id, max_gen_length=5, device=DEVICE)\n",
        "\n",
        "        # Try different parsing methods\n",
        "        pred_raw = gen_text.split(\"Label:\")[-1].strip()\n",
        "\n",
        "        # Extract first digit if present\n",
        "        import re\n",
        "        digits = re.findall(r'\\d+', pred_raw)\n",
        "        pred_digit = digits[0] if digits else \"NO_DIGIT\"\n",
        "\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"  Full generated text: {repr(gen_text)}\")\n",
        "        print(f\"  After 'Label:': {repr(pred_raw)}\")\n",
        "        print(f\"  Extracted digit: {pred_digit}\")\n",
        "        print(f\"  True label: {true_label}\")\n",
        "        print(f\"  Match: {pred_digit == true_label}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"If you see 'NO_DIGIT' or mismatches, the model isn't generating\")\n",
        "print(\"the expected format. Try increasing max_gen_length or check training.\")\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "id": "4kuf5U2zv0jK"
      },
      "id": "4kuf5U2zv0jK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3656cf81",
      "metadata": {
        "id": "3656cf81"
      },
      "outputs": [],
      "source": [
        "# Sanity check: after fine-tuning, the accuracy should be better than random guessing (33.33%)\n",
        "# The accuracy we got is around 77.96% using whole training data and 1 epoch\n",
        "SAVE_DIR = \"best_model\"\n",
        "finetuned_model = GPT2Model(GPT2Config()).to(DEVICE)\n",
        "finetuned_model.load_state_dict(torch.load(f\"{SAVE_DIR}/model.pt\"))\n",
        "test_loader = DataLoader(test_dataset,shuffle=False,collate_fn=XNLIDataset.collate_fn)\n",
        "acc, all_preds, all_labels = evaluate_gpt2_xnli(finetuned_model, tokenizer, test_loader, max_gen_length=1, device=DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "354805ec",
      "metadata": {
        "id": "354805ec"
      },
      "source": [
        "# Task 3: Multilingual NLI with GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd0ee4ba",
      "metadata": {
        "id": "fd0ee4ba"
      },
      "source": [
        "In this task, you will:\n",
        "\n",
        "- Test the fine-tuned GPT-2 on non-English languages for zero-shot cross-lingual transfer.\n",
        "\n",
        "- For each non-English language, fine-tune a model on the corresponding training set.\n",
        "\n",
        "- Fine-tune a unified model on the training sets of all languages."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd57fdf1",
      "metadata": {
        "id": "dd57fdf1"
      },
      "source": [
        "## Zero-shot Cross-lingual Transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a247f1d0",
      "metadata": {
        "id": "a247f1d0"
      },
      "outputs": [],
      "source": [
        "langs = ['en', 'ar', 'bg', 'de','el','es','fr','hi','ru','sw','th','tr','ur','vi','zh']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff1f58b2",
      "metadata": {
        "id": "ff1f58b2"
      },
      "outputs": [],
      "source": [
        "TEST_SUBSET = 1\n",
        "finetuned_model = GPT2Model(GPT2Config()).to(DEVICE)\n",
        "finetuned_model.load_state_dict(torch.load(path))\n",
        "all_test_datasets = {}\n",
        "all_test_loader = {}\n",
        "for lang in langs:\n",
        "    test_dataset = XNLIDataset(split=\"test\", lang=lang, tokenizer=tokenizer, max_length=1024, subset=TEST_SUBSET)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=XNLIDataset.collate_fn)\n",
        "    all_test_datasets[lang] = test_dataset\n",
        "    all_test_loader[lang] = test_loader\n",
        "\n",
        "all_results = {}\n",
        "for lang in langs:\n",
        "    test_loader = all_test_loader[lang]\n",
        "    if lang == \"en\":\n",
        "        print(f\"Evaluating on {lang}...\")\n",
        "    else:\n",
        "        print(f\"Evaluating zero-shot on {lang}...\")\n",
        "    acc, all_preds, all_labels = evaluate_gpt2_xnli(finetuned_model, tokenizer, test_loader, max_gen_length=1, device=DEVICE)\n",
        "    all_results[lang] = acc\n",
        "\n",
        "print(\"Zero-shot cross-lingual accuracy per language:\")\n",
        "for lang, acc in all_results.items():\n",
        "    print(f\"{lang}: {acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "491d87e2",
      "metadata": {
        "id": "491d87e2"
      },
      "source": [
        "## Fertility-based Language Selection\n",
        "\n",
        "Guidance: You may notice that some languages achieve reasonable zero-shot cross-lingual performance. This is likely because these languages are closer to English (e.g., in writing system), making cross-lingual transfer from English easier. However, many other languages perform close to random guessing, which is expected since GPT-2 was pretrained entirely on English data.\n",
        "\n",
        "To perform further multilingual fine-tuning, we need to identify which languages GPT-2 can realistically support (because if a language is not supported, fine-tuning on it will have little effect). A straightforward way to check this is to inspect the tokens in the models tokenizer. However, this is not practical for GPT-2-like models, because they use a Byte-Pair Encoding (BPE) tokenizer. BPE can decompose any Unicode string into subwords, even if the string never appeared in training, making it difficult to determine whether a language is truly supported.\n",
        "\n",
        "Instead, we can approximate tokenizer support using fertility, a metric that measures the average number of subwords produced per word. Lower fertility indicates better tokenizer quality and compression, while high fertility suggests heavy fragmentation, which can hurt model performance. By combining fertility analysis with zero-shot cross-lingual results, we can identify a subset of languages that GPT-2 can reasonably handle (a rough estimate, as officially GPT-2 is designed for English). Then, we can proceed with multilingual fine-tuning experiments on these languages.\n",
        "\n",
        "Reference: How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d28acaf7",
      "metadata": {
        "id": "d28acaf7"
      },
      "outputs": [],
      "source": [
        "def compute_fertility(dataset, tokenizer):\n",
        "    \"\"\"\n",
        "    Compute average fertility for a dataset.\n",
        "    Fertility = #tokens / #words\n",
        "    Note: word splitting is approximate and uses whitespace.\n",
        "    \"\"\"\n",
        "    total_words = 0\n",
        "    total_tokens = 0\n",
        "    samples = len(dataset)\n",
        "\n",
        "    for i in tqdm(range(samples), desc=\"Computing fertility\"):\n",
        "        row = dataset.data.iloc[i]\n",
        "        for sent in [row['premise'], row['hypo']]:\n",
        "            words = sent.strip().split()  # crude word estimate\n",
        "            tokens = tokenizer.tokenize(sent)\n",
        "            total_words += len(words)\n",
        "            total_tokens += len(tokens)\n",
        "\n",
        "    fertility = total_tokens / total_words if total_words > 0 else 0.0\n",
        "    return fertility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ce5b291",
      "metadata": {
        "id": "6ce5b291"
      },
      "outputs": [],
      "source": [
        "subset_for_check = 0.01\n",
        "\n",
        "for lang in langs:\n",
        "    train_dataset = XNLIDataset(\n",
        "        split=\"train\",\n",
        "        lang=lang,\n",
        "        tokenizer=tokenizer,\n",
        "        subset=subset_for_check\n",
        "    )\n",
        "    fertility_score = compute_fertility(train_dataset, tokenizer)\n",
        "    print(f\"{lang}: fertility = {fertility_score:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37090f47",
      "metadata": {
        "id": "37090f47"
      },
      "source": [
        "## Fine-tune GPT-2 (per-language)\n",
        "\n",
        "Guidance: Load the pretrained GPT-2 (not the ones fine-tuned on English NLI) along with the training data for a single target language. Choose non-English languages that performed well in the zero-shot cross-lingual transfer and fertility evaluation. It depends on you how many languages to include. Fine-tune a separate model for each selected language. Afterwards, compare these per-language fine-tuned models with the zero-shot cross-lingual transfer results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2cb1bee",
      "metadata": {
        "id": "c2cb1bee"
      },
      "source": [
        "## Fine-tune GPT-2 (all)\n",
        "\n",
        "Guidance: Load the pretrained GPT-2 (again, not the ones fine-tuned on English NLI) along with the training data for all target languages, including English. For non-English languages, select those that performed well in the zero-shot cross-lingual transfer and fertility evaluation. It depends on you how many languages to include. Fine-tune a single model on this combined multilingual dataset. Afterwards, compare this model with the per-language fine-tuned models and the zero-shot cross-lingual transfer results."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}