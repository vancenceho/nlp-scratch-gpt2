{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96361c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "319.83s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Dependencies installed successfully.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt\n",
    "print(\"Dependencies installed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2903822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vancence/Desktop/kp-gpt2-nlp/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, dtype\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from config import PretrainedConfig, GPT2Config\n",
    "from transformers import GPT2Model as OpenAIGPT2Model\n",
    "from transformers import GPT2Tokenizer\n",
    "from utils import *\n",
    "from einops import rearrange\n",
    "from typing import Callable, Iterable, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23384a93",
   "metadata": {},
   "source": [
    "# Task 1: Implement GPT-2\n",
    "In this task, you will:\n",
    "- Load the GPT-2 tokenizer.\n",
    "- Implement the GPT-2 model.\n",
    "- Implement the Adam optimizer.\n",
    "- Conduct a toy pretraining of GPT-2 on the provided small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e498552",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb30c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [14618, 11, 428, 318, 262, 3726, 286, 4277, 2457, 1628, 0]\n",
      "token Welcome\n",
      "token ,\n",
      "token  this\n",
      "token  is\n",
      "token  the\n",
      "token  beginning\n",
      "token  of\n",
      "token  default\n",
      "token  final\n",
      "token  project\n",
      "token !\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "text = \"Welcome, this is the beginning of default final project!\"\n",
    "input_ids = tokenizer(text)['input_ids']\n",
    "print('input_ids:', input_ids)\n",
    "for token in input_ids:\n",
    "    print('token', tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd74b99",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061db6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_attention_heads = config.num_attention_heads\n",
    "    self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "    self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "    # Initialize the linear transformation layers for key, value, query.\n",
    "    self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "    self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "    self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "    # This dropout is applied to normalized attention scores following the original\n",
    "    # implementation of transformer. Although it is a bit unusual, we empirically\n",
    "    # observe that it yields better performance.\n",
    "    self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "  def transform(self, x, linear_layer):\n",
    "    # The corresponding linear_layer of k, v, q are used to project the hidden_state (x).\n",
    "    proj = linear_layer(x)\n",
    "    # Next, we need to produce multiple heads for the proj. This is done by spliting the\n",
    "    # hidden state to self.num_attention_heads, each of size self.attention_head_size.\n",
    "    proj = rearrange(proj, 'b t (h d) -> b t h d', h=self.num_attention_heads)\n",
    "    # By proper transpose, we have proj of size [bs, num_attention_heads, seq_len, attention_head_size].\n",
    "    proj = rearrange(proj, 'b t h d -> b h t d')\n",
    "    return proj\n",
    "\n",
    "  def attention(self, key, query, value, attention_mask):\n",
    "    \"\"\"\n",
    "    TODO-1: Compute scaled dot-product attention for GPT-2.\n",
    "\n",
    "    Implementation hints:\n",
    "    1. Compute raw attention scores using QK^T, and scale them by sqrt(d_k).\n",
    "    2. Apply a causal mask (lower-triangular) to prevent attending to future tokens.\n",
    "    3. Optionally add the external attention_mask (e.g., padding positions).\n",
    "    4. Normalize the scores with softmax to obtain attention probabilities.\n",
    "    5. Apply dropout on the probabilities.\n",
    "    6. Use them to weight the values (V) and obtain the context vectors.\n",
    "    7. Finally, merge all attention heads back into a single hidden representation.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    # Compute raw attention scores using QK^T, and scale them by sqrt(d_k).\n",
    "    # q, k, v shapes: [B, H, T, d]\n",
    "    raw_scores = torch.matmul(query, key.transpose(-2, -1))  # [B, H, T, T]\n",
    "    d_k = query.size(-1)\n",
    "    scaled_scores = raw_scores / (torch.sqrt(torch.tensor(d_k, dtype=torch.float32)))\n",
    "\n",
    "    # Apply a causal mask over the token dimension (lower triangular over T x T).\n",
    "    Tq = query.size(-2)\n",
    "    Tk = key.size(-2)\n",
    "    causal = torch.tril(torch.ones((Tq, Tk), device=query.device, dtype=torch.bool))  # [Tq, Tk]; 1 for keep, 0 for mask; create a lower triangular mask\n",
    "    scaled_scores = scaled_scores.masked_fill(~causal, torch.finfo(scaled_scores.dtype).min)  # Fill masked positions with -inf\n",
    "\n",
    "    # Optionally add the external attention_mask (e.g., padding positions).\n",
    "    if attention_mask is not None:\n",
    "        # attention_mask is already in logit space: 0 for keep, large negative for mask\n",
    "        scaled_scores = scaled_scores + attention_mask\n",
    "    \n",
    "    # Normalize the scores with softmax to obtain attention probabilities.\n",
    "    attention_probs = F.softmax(scaled_scores, dim=-1)\n",
    "\n",
    "    # Apply dropout to the probabilities.\n",
    "    attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "    # Weight the values (V) and obtain the context vectors.\n",
    "    context_vectors = torch.matmul(attention_probs, value)  # [B, H, T, d]\n",
    "\n",
    "    # Merge all attention heads back into a single hidden representation.\n",
    "    context_vectors = rearrange(context_vectors, 'b h t d -> b t (h d)')\n",
    "\n",
    "    return context_vectors\n",
    "    \n",
    " \n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "  def forward(self, hidden_states, attention_mask):\n",
    "    \"\"\"\n",
    "    hidden_states: [bs, seq_len, hidden_state]\n",
    "    attention_mask: [bs, 1, 1, seq_len]\n",
    "    output: [bs, seq_len, hidden_state]\n",
    "    \"\"\"\n",
    "    # First, we have to generate the key, value, query for each token for multi-head attention\n",
    "    # using self.transform (more details inside the function).\n",
    "    # Size of *_layer is [bs, num_attention_heads, seq_len, attention_head_size].\n",
    "    key_layer = self.transform(hidden_states, self.key)\n",
    "    value_layer = self.transform(hidden_states, self.value)\n",
    "    query_layer = self.transform(hidden_states, self.query)\n",
    "    \n",
    "    # Calculate the multi-head attention using the self.attention function.\n",
    "    attn_value = self.attention(key_layer, query_layer, value_layer, attention_mask)\n",
    "    return attn_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc9ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Layer(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    # Multi-head attention.\n",
    "    self.self_attention = CausalSelfAttention(config)\n",
    "    # Add-norm for multi-head attention.\n",
    "    self.attention_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "    self.attention_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "    self.attention_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    # Feed forward.\n",
    "    self.interm_dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "    self.interm_af = F.gelu\n",
    "    # Add-norm for feed forward.\n",
    "    self.out_dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "    self.out_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "    self.out_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "  def add(self, input, output, dense_layer, dropout):\n",
    "    \"\"\"\n",
    "    TODO-2: Residual connection with dense projection and dropout.\n",
    "    \n",
    "    Implementation hints:\n",
    "    1. Project the 'output' through dense_layer.\n",
    "    2. Apply dropout to prevent overfitting.\n",
    "    3. Add the original 'input' (residual connection) to the processed output.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    # Apply dense projection to the output.\n",
    "    output = dense_layer(output)\n",
    "\n",
    "    # Apply dropout to prevent overfitting. \n",
    "    output = dropout(output)\n",
    "\n",
    "    # Add original 'input' (residual connection) to the processed output.\n",
    "    output = input + output\n",
    "\n",
    "    return output\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "  def forward(self, hidden_states, attention_mask):\n",
    "    \"\"\"\n",
    "    TODO-3: Forward pass of a GPT-2 layer.\n",
    "\n",
    "    Implementation hints:\n",
    "    ---- Self-Attention Block ----\n",
    "    1. LayerNorm the input for stability using self.attention_layer_norm.\n",
    "    2. Compute multi-head causal self-attention using self.self_attention.\n",
    "    3. Apply residual connection using self.add (dense_layer=self.attention_dense, dropout=self.attention_dropout).\n",
    "\n",
    "    ---- Feed Forward Block ----\n",
    "    4. LayerNorm the hidden_states from attention block using self.out_layer_norm.\n",
    "    5. Pass through a two-layer feed-forward network with activation:\n",
    "       self.interm_dense -> self.interm_af -> self.out_dense\n",
    "    6. Apply residual connection again using self.add (dense_layer=self.out_dense, dropout=self.out_dropout).\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    # ---- Self-Attention Block (Pre-LN) ----\n",
    "    residual = hidden_states\n",
    "    # LayerNorm the input for stability using self.attention_layer_norm.\n",
    "    normed = self.attention_layer_norm(hidden_states)\n",
    "    # Compute multi-head causal self-attention using self.self_attention.\n",
    "    attention_output = self.self_attention(normed, attention_mask)\n",
    "    # Apply residual connection using self.add (dense_layer=self.attention_dense, dropout=self.attention_dropout).\n",
    "    hidden_states = self.add(residual, attention_output, self.attention_dense, self.attention_dropout)\n",
    "\n",
    "    # ---- Feed Forward Block (Pre-LN) ----\n",
    "    residual = hidden_states\n",
    "    # LayerNorm the hidden_states from attention block using self.out_layer_norm.\n",
    "    normed = self.out_layer_norm(hidden_states)\n",
    "    # Pass through a two-layer feed-forward network with activation: self.interm_dense -> self.interm_af -> self.out_dense\n",
    "    ffn_preproj = self.interm_dense(normed)\n",
    "    ffn_preproj = self.interm_af(ffn_preproj)\n",
    "    # Apply residual connection again using self.add (dense_layer=self.out_dense, dropout=self.out_dropout).\n",
    "    hidden_states = self.add(residual, ffn_preproj, self.out_dense, self.out_dropout)\n",
    "\n",
    "    return hidden_states\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a04146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTPreTrainedModel(nn.Module):\n",
    "\n",
    "  def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n",
    "    super().__init__()\n",
    "    self.config = config\n",
    "    self.name_or_path = config.name_or_path\n",
    "\n",
    "  def init_weights(self):\n",
    "    # Initialize weights\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def _init_weights(self, module):\n",
    "    \"\"\" Initialize the weights \"\"\"\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "      # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "      # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "      module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "      module.bias.data.zero_()\n",
    "      module.weight.data.fill_(1.0)\n",
    "    if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "      module.bias.data.zero_()\n",
    "\n",
    "  @property\n",
    "  def dtype(self) -> dtype:\n",
    "    return get_parameter_dtype(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68d1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Model(GPTPreTrainedModel):\n",
    "  \"\"\"\n",
    "  The GPT model returns the final embeddings for each token in a sentence.\n",
    "\n",
    "  The model consists of:\n",
    "  1. Embedding layers (used in self.embed).\n",
    "  2. A stack of n GPT layers (used in self.encode).\n",
    "  3. A linear transformation layer for the [CLS] token (used in self.forward, as given).\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__(config)\n",
    "    self.config = config\n",
    "\n",
    "    # Embedding layers.\n",
    "    self.word_embedding = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "    self.pos_embedding = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "    self.embed_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    # Register position_ids (1, len position emb) to buffer because it is a constant.\n",
    "    position_ids = torch.arange(config.max_position_embeddings).unsqueeze(0)\n",
    "    self.register_buffer('position_ids', position_ids)\n",
    "\n",
    "    # GPT-2 layers.\n",
    "    self.gpt_layers = nn.ModuleList([GPT2Layer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    # [CLS] token transformations.\n",
    "    self.pooler_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "    self.pooler_af = nn.Tanh()\n",
    "\n",
    "    # Final layer norm.\n",
    "    self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    self.init_weights()\n",
    "\n",
    "  def embed(self, input_ids):\n",
    "    \"\"\"\n",
    "    TODO-4: Embedding layer of the GPT-2 model.\n",
    "\n",
    "    Implementation hints:\n",
    "    1. Use self.word_embedding to convert input_ids to embeddings.\n",
    "    2. Generate position ids and convert to embeddings using self.pos_embedding.\n",
    "    3. Sum token and position embeddings.\n",
    "    4. Apply self.embed_dropout to the sum.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    # Use self.word_embedding to convert input_ids to embeddings.\n",
    "    embeddings = self.word_embedding(input_ids)\n",
    "\n",
    "    # Generate position ids and convert to embeddings using self.pos_embedding.\n",
    "    position_ids = self.position_ids[:, :input_ids.shape[1]]\n",
    "    position_embeddings = self.pos_embedding(position_ids)\n",
    "\n",
    "    # Sum token and position embeddings.\n",
    "    embeddings = embeddings + position_embeddings\n",
    "\n",
    "    # Apply self.embed_dropout to the sum.\n",
    "    embeddings = self.embed_dropout(embeddings)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "  def encode(self, hidden_states, attention_mask):\n",
    "    \"\"\"\n",
    "    hidden_states: the output from the embedding layer [batch_size, seq_len, hidden_size]\n",
    "    attention_mask: [batch_size, seq_len]\n",
    "    \"\"\"\n",
    "    # Get the extended attention mask for self-attention.\n",
    "    # Returns extended_attention_mask of size [batch_size, 1, 1, seq_len].\n",
    "    # Distinguishes between non-padding tokens (with a value of 0) and padding tokens\n",
    "    # (with a value of a large negative number).\n",
    "    extended_attention_mask: torch.Tensor = get_extended_attention_mask(attention_mask, self.dtype)\n",
    "\n",
    "    # Pass the hidden states through the encoder layers.\n",
    "    for i, layer_module in enumerate(self.gpt_layers):\n",
    "      # Feed the encoding from the last bert_layer to the next.\n",
    "      hidden_states = layer_module(hidden_states, extended_attention_mask)\n",
    "\n",
    "    return hidden_states\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    input_ids: [batch_size, seq_len], seq_len is the max length of the batch\n",
    "    attention_mask: same size as input_ids, 1 represents non-padding tokens, 0 represents padding tokens\n",
    "    \"\"\"\n",
    "    # Get the embedding for each input token.\n",
    "    embedding_output = self.embed(input_ids=input_ids)\n",
    "\n",
    "    # Feed to a transformer (a stack of GPTLayers).\n",
    "    sequence_output = self.encode(embedding_output, attention_mask=attention_mask)\n",
    "    sequence_output = self.final_layer_norm(sequence_output)\n",
    "\n",
    "    # Get the hidden state of the final token.\n",
    "    last_non_pad_idx = attention_mask.sum(dim=1) - 1  # Subtract 1 to get last index\n",
    "    last_token = sequence_output[torch.arange(sequence_output.shape[0]), last_non_pad_idx]\n",
    "\n",
    "    return {'last_hidden_state': sequence_output, 'last_token': last_token}\n",
    "\n",
    "  def hidden_state_to_token(self, hidden_state):\n",
    "    \"\"\"\n",
    "    TODO-5: Convert hidden states back to token logits.\n",
    "\n",
    "    Implementation hints: \n",
    "    - GPT-2 uses weight tying with the input word embeddings. \n",
    "    - The logits are the dot product between output hidden states and the word embedding weights: hidden_state(s) * E^T\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    # Compute the dot product between the hidden states and the word embedding weights: hidden_state(s) * E^T\n",
    "    logits = torch.matmul(hidden_state, self.word_embedding.weight.T)\n",
    "\n",
    "    return logits\n",
    "    \n",
    "    raise NotImplementedError\n",
    "\n",
    "  @classmethod\n",
    "  def from_pretrained(cls, model='gpt2', d=768, l=12, num_heads=12):\n",
    "    gpt_model = OpenAIGPT2Model.from_pretrained(model).eval()\n",
    "    our_model = GPT2Model(GPT2Config(hidden_size=d, num_hidden_layers=l,num_attention_heads=num_heads,\n",
    "                                     intermediate_size=d*3)).eval()\n",
    "\n",
    "    # Load word and positional embeddings.\n",
    "    our_model.word_embedding.load_state_dict(gpt_model.wte.state_dict())\n",
    "    our_model.pos_embedding.load_state_dict(gpt_model.wpe.state_dict())\n",
    "\n",
    "    for i in range(l):\n",
    "      l = our_model.gpt_layers[i]\n",
    "      # Remap the Q,K,V weights from a conv1d to 3 linear projections\n",
    "      l.self_attention.query.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.weight'][:, :d].T\n",
    "      l.self_attention.query.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.bias'][:d]\n",
    "      l.self_attention.key.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.weight'][:, d:d*2].T\n",
    "      l.self_attention.key.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.bias'][d:d*2]\n",
    "      l.self_attention.value.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.weight'][:, d*2:].T\n",
    "      l.self_attention.value.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.bias'][d*2:]\n",
    "\n",
    "      # Remap final dense layer in MHA.\n",
    "      l.attention_dense.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_proj.weight'].T\n",
    "      l.attention_dense.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_proj.bias']\n",
    "\n",
    "      # Remap attention layer norm.\n",
    "      l.attention_layer_norm.weight.data = gpt_model.state_dict()[f'h.{i}.ln_1.weight']\n",
    "      l.attention_layer_norm.bias.data = gpt_model.state_dict()[f'h.{i}.ln_1.bias']\n",
    "\n",
    "      # Remap post-attention MLP layers.\n",
    "      l.interm_dense.weight.data = gpt_model.state_dict()[f'h.{i}.mlp.c_fc.weight'].T\n",
    "      l.interm_dense.bias.data = gpt_model.state_dict()[f'h.{i}.mlp.c_fc.bias']\n",
    "      l.out_dense.weight.data = gpt_model.state_dict()[f'h.{i}.mlp.c_proj.weight'].T\n",
    "      l.out_dense.bias.data = gpt_model.state_dict()[f'h.{i}.mlp.c_proj.bias']\n",
    "\n",
    "      # Remap second layer norm weights.\n",
    "      l.out_layer_norm.weight.data = gpt_model.state_dict()[f'h.{i}.ln_2.weight']\n",
    "      l.out_layer_norm.bias.data = gpt_model.state_dict()[f'h.{i}.ln_2.bias']\n",
    "\n",
    "    # Remap the final layer norm values.\n",
    "    our_model.final_layer_norm.weight.data = gpt_model.state_dict()['ln_f.weight']\n",
    "    our_model.final_layer_norm.bias.data = gpt_model.state_dict()['ln_f.bias']\n",
    "\n",
    "    return our_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c95668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your GPT2 implementation is correct!\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: compare with Huggingface GPT2 implementation\n",
    "def test_gpt2(model_size='gpt2'):\n",
    "  sent_ids = torch.tensor([[101, 7592, 2088, 102, 0, 0, 0, 0],\n",
    "                           [101, 7592, 15756, 2897, 2005, 17953, 2361, 102]])\n",
    "  att_mask = torch.tensor([[1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "\n",
    "  # Load both the OpenAI and your own model.\n",
    "  openai_model = OpenAIGPT2Model.from_pretrained(model_size)\n",
    "  gpt = GPT2Model.from_pretrained(model=model_size, **model_size_to_params(model_size))\n",
    "\n",
    "  outputs = gpt(sent_ids, att_mask)\n",
    "  openai_outputs = openai_model(input_ids=sent_ids, attention_mask=att_mask, output_hidden_states=True).hidden_states[-1]\n",
    "\n",
    "  att_mask = att_mask.unsqueeze(-1)\n",
    "  outputs['last_hidden_state'] = outputs['last_hidden_state'] * att_mask\n",
    "  openai_outputs *= att_mask\n",
    "\n",
    "  assert torch.allclose(outputs['last_hidden_state'], openai_outputs, atol=1e-1, rtol=1e-2)\n",
    "\n",
    "  print(\"Your GPT2 implementation is correct!\")\n",
    "\n",
    "test_gpt2('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a449a7",
   "metadata": {},
   "source": [
    "## Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35494be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW(Optimizer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            params: Iterable[torch.nn.parameter.Parameter],\n",
    "            lr: float = 1e-3,\n",
    "            betas: Tuple[float, float] = (0.9, 0.999),\n",
    "            eps: float = 1e-6,\n",
    "            weight_decay: float = 0.0,\n",
    "            correct_bias: bool = True,\n",
    "    ):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(eps))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, correct_bias=correct_bias)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure: Callable = None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
    "\n",
    "                # State should be stored in this dictionary.\n",
    "                state = self.state[p]\n",
    "\n",
    "                # Access hyperparameters from the `group` dictionary.\n",
    "                lr = group[\"lr\"]\n",
    "                eps = group[\"eps\"]\n",
    "                weight_decay = group[\"weight_decay\"]\n",
    "                correct_bias = group[\"correct_bias\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "                \n",
    "                state[\"step\"] += 1\n",
    "                t = state[\"step\"]\n",
    "                \n",
    "                \"\"\"\n",
    "                TODO-6: Implement the AdamW parameter update for this step.\n",
    "\n",
    "                Implementation hints:\n",
    "                1. Update biased first moment estimate:\n",
    "                    m_t = beta1 * m_{t-1} + (1 - beta1) * grad\n",
    "                2. Update biased second raw moment estimate:\n",
    "                    v_t = beta2 * v_{t-1} + (1 - beta2) * grad^2\n",
    "                3. Apply bias correction if correct_bias=True:\n",
    "                    m_hat = m_t / (1 - beta1^t)\n",
    "                    v_hat = v_t / (1 - beta2^t)\n",
    "                4. Compute step size:\n",
    "                    step_size = lr (or lr / (1 - beta1^t) if bias correction)\n",
    "                5. Update parameters:\n",
    "                    p = p - step_size * m_hat / (sqrt(v_hat) + eps)\n",
    "                6. Apply decoupled weight decay after the parameter update (if weight_decay > 0):\n",
    "                    p = p - lr * weight_decay * p\n",
    "                Reference:\n",
    "                Algorithm 1 in \"Adam: A Method for Stochastic Optimization\"\n",
    "                https://arxiv.org/abs/1412.6980\n",
    "                \"\"\"\n",
    "                ### YOUR CODE HERE\n",
    "\n",
    "                # Update biased first moment estimate: m_t = beta1 * m_{t-1} + (1 - beta1) * grad\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                # Update biased second raw moment estimate: v_t = beta2 * v_{t-1} + (1 - beta2) * grad^2\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                # Apply bias correction if correct_bias=True:\n",
    "                if correct_bias:\n",
    "                    # m_hat = m_t / (1 - beta1^t)\n",
    "                    bias_correction1 = 1 - beta1 ** t\n",
    "                    # v_hat = v_t / (1 - beta2^t)\n",
    "                    bias_correction2 = 1 - beta2 ** t\n",
    "                    # Compute step size: step_size = lr / (1 - beta1^t) if bias correction\n",
    "                    step_size = lr * (bias_correction2 ** 0.5) / bias_correction1\n",
    "                else:\n",
    "                    # Compute step size: step_size = lr\n",
    "                    step_size = lr\n",
    "\n",
    "                # Update parameters: p = p - step_size * m_hat / (sqrt(v_hat) + eps)\n",
    "                # Denominator = sqrt(v_hat) + eps\n",
    "                denominator = exp_avg_sq.sqrt().add_(eps)\n",
    "                # Parameter update: p = p - step_size * m_hat / (sqrt(v_hat) + eps)\n",
    "                p.data.addcdiv_(exp_avg, denominator, value=-step_size)\n",
    "\n",
    "                # Apply decoupled weight decay after the parameter update (if weight_decay > 0):\n",
    "                if weight_decay > 0:\n",
    "                    p.data.add_(p.data, alpha=-lr * weight_decay)\n",
    "        return loss\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544e2a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5548,  0.8667,  0.0729],\n",
      "        [-0.4472, -0.2951, -0.2717]])\n",
      "tensor([[ 0.5548,  0.8667,  0.0729],\n",
      "        [-0.4472, -0.2951, -0.2717]])\n",
      "Optimizer test passed!\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for AdamW optimizer\n",
    "def test_optimizer(opt_class) -> torch.Tensor:\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    model = torch.nn.Linear(3, 2, bias=False)\n",
    "    opt = opt_class(\n",
    "        model.parameters(),\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        correct_bias=True,\n",
    "    )\n",
    "    for i in range(1000):\n",
    "        opt.zero_grad()\n",
    "        x = torch.FloatTensor(rng.uniform(size=[model.in_features]))\n",
    "        y_hat = model(x)\n",
    "        y = torch.Tensor([x[0] + x[1], -x[2]])\n",
    "        loss = ((y - y_hat) ** 2).sum()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return model.weight.detach()\n",
    "\n",
    "SEED = 0\n",
    "ref = torch.tensor(np.load(\"optimizer_test.npy\"))\n",
    "actual = test_optimizer(AdamW)\n",
    "print(ref)\n",
    "print(actual)\n",
    "assert torch.allclose(ref, actual, atol=1e-6, rtol=1e-4)\n",
    "print(\"Optimizer test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bde426b",
   "metadata": {},
   "source": [
    "## Toy GPT-2 Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03fffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for preparing text data for language model training.\n",
    "\n",
    "    Each line in the input text file is treated as a separate training example.\n",
    "    The dataset uses a tokenizer to convert text into input IDs and attention masks,\n",
    "    with optional truncation and padding to a fixed maximum sequence length.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the text file. Each line is a separate sample.\n",
    "        tokenizer (PreTrainedTokenizer): Tokenizer to convert text to token IDs.\n",
    "        max_len (int): Maximum sequence length; sequences longer than this are truncated,\n",
    "                       shorter sequences are padded.\n",
    "\n",
    "    Returns per item:\n",
    "        input_ids (torch.Tensor): Token IDs of shape [max_len].\n",
    "        attention_mask (torch.Tensor): Attention mask of shape [max_len], 1 for real tokens, 0 for padding.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath, tokenizer, max_len):\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.texts = f.read().splitlines()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = enc['input_ids'].squeeze(0)\n",
    "        attention_mask = enc['attention_mask'].squeeze(0)\n",
    "        return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34229c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamter of toy gpt2 pretraining\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "CORRECT_BIAS = True\n",
    "HIDDEN_SIZE = 128 # 768 for gpt2\n",
    "NUM_HIDDEN_LAYERS = 2 # 12 for gpt2\n",
    "NUM_ATTENTION_HEADS = 4 # 12 for gpt2\n",
    "MAX_SEQ_LEN = 128 # 1024 for gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad3e6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train | Epoch 0 | Batch 0 | Global Avg Train Loss: 10.5164\n",
      "Train | Epoch 0 | Batch 10 | Global Avg Train Loss: 8.9540\n",
      "Train | Epoch 0 | Batch 20 | Global Avg Train Loss: 8.3768\n",
      "Train | Epoch 0 | Batch 30 | Global Avg Train Loss: 7.5964\n",
      "Train | Epoch 0 | Batch 40 | Global Avg Train Loss: 6.8495\n",
      "Train | Epoch 0 | Batch 50 | Global Avg Train Loss: 6.2175\n",
      "Train | Epoch 0 | Batch 60 | Global Avg Train Loss: 5.7365\n",
      "Train | Epoch 0 | Batch 70 | Global Avg Train Loss: 5.4084\n",
      "Train | Epoch 0 | Batch 80 | Global Avg Train Loss: 5.1669\n",
      "Train | Epoch 0 | Batch 90 | Global Avg Train Loss: 4.9807\n",
      "Train | Epoch 0 | Batch 100 | Global Avg Train Loss: 4.8170\n",
      "Train | Epoch 0 | Batch 110 | Global Avg Train Loss: 4.6877\n",
      "Train | Epoch 0 | Batch 120 | Global Avg Train Loss: 4.6141\n",
      "Train | Epoch 0 | Batch 130 | Global Avg Train Loss: 4.4938\n",
      "Train | Epoch 0 | Batch 140 | Global Avg Train Loss: 4.4470\n",
      "Train | Epoch 0 | Batch 150 | Global Avg Train Loss: 4.3866\n",
      "Train | Epoch 0 | Batch 160 | Global Avg Train Loss: 4.2962\n",
      "Train | Epoch 0 | Batch 170 | Global Avg Train Loss: 4.2373\n",
      "Train | Epoch 0 | Batch 180 | Global Avg Train Loss: 4.1884\n",
      "Train | Epoch 0 | Batch 190 | Global Avg Train Loss: 4.1559\n",
      "Train | Epoch 0 | Batch 200 | Global Avg Train Loss: 4.0955\n",
      "Train | Epoch 0 | Batch 210 | Global Avg Train Loss: 4.0498\n",
      "Train | Epoch 0 | Batch 220 | Global Avg Train Loss: 4.0332\n",
      "Train | Epoch 0 | Batch 230 | Global Avg Train Loss: 3.9997\n",
      "Train | Epoch 0 | Batch 240 | Global Avg Train Loss: 3.9708\n",
      "Train | Epoch 0 | Batch 250 | Global Avg Train Loss: 3.9589\n",
      "Train | Epoch 0 | Batch 260 | Global Avg Train Loss: 3.9441\n",
      "Train | Epoch 0 | Batch 270 | Global Avg Train Loss: 3.9429\n",
      "Train | Epoch 0 | Batch 280 | Global Avg Train Loss: 3.9008\n",
      "Train | Epoch 0 | Batch 290 | Global Avg Train Loss: 3.8825\n",
      "Train | Epoch 0 | Batch 300 | Global Avg Train Loss: 3.8643\n",
      "Train | Epoch 0 | Batch 310 | Global Avg Train Loss: 3.8537\n",
      "Train | Epoch 0 | Batch 320 | Global Avg Train Loss: 3.8455\n",
      "Train | Epoch 0 | Batch 330 | Global Avg Train Loss: 3.8219\n",
      "Train | Epoch 0 | Batch 340 | Global Avg Train Loss: 3.8098\n",
      "Train | Epoch 0 | Batch 350 | Global Avg Train Loss: 3.7890\n",
      "Train | Epoch 0 | Batch 360 | Global Avg Train Loss: 3.7698\n",
      "Train | Epoch 0 | Batch 370 | Global Avg Train Loss: 3.7494\n",
      "Train | Epoch 0 | Batch 380 | Global Avg Train Loss: 3.7494\n",
      "Train | Epoch 0 | Batch 390 | Global Avg Train Loss: 3.7351\n",
      "Train | Epoch 0 | Batch 400 | Global Avg Train Loss: 3.7215\n",
      "Train | Epoch 0 | Batch 410 | Global Avg Train Loss: 3.7151\n",
      "Train | Epoch 0 | Batch 420 | Global Avg Train Loss: 3.6997\n",
      "Train | Epoch 0 | Batch 430 | Global Avg Train Loss: 3.6847\n",
      "Train | Epoch 0 | Batch 440 | Global Avg Train Loss: 3.6570\n",
      "Train | Epoch 0 | Batch 450 | Global Avg Train Loss: 3.6481\n",
      "Train | Epoch 0 | Batch 460 | Global Avg Train Loss: 3.6335\n",
      "Train | Epoch 0 | Batch 470 | Global Avg Train Loss: 3.6372\n",
      "Train | Epoch 0 | Batch 480 | Global Avg Train Loss: 3.6212\n",
      "Train | Epoch 0 | Batch 490 | Global Avg Train Loss: 3.6217\n",
      "Train | Epoch 0 | Batch 500 | Global Avg Train Loss: 3.6161\n",
      "Train | Epoch 0 | Batch 510 | Global Avg Train Loss: 3.6011\n",
      "Train | Epoch 0 | Batch 520 | Global Avg Train Loss: 3.5883\n",
      "Train | Epoch 0 | Batch 530 | Global Avg Train Loss: 3.5750\n",
      "Train | Epoch 0 | Batch 540 | Global Avg Train Loss: 3.5652\n",
      "Train | Epoch 0 | Batch 550 | Global Avg Train Loss: 3.5608\n",
      "Train | Epoch 0 | Batch 560 | Global Avg Train Loss: 3.5459\n",
      "Train | Epoch 0 | Batch 570 | Global Avg Train Loss: 3.5309\n",
      "Train | Epoch 0 | Batch 580 | Global Avg Train Loss: 3.5257\n",
      "Train | Epoch 0 | Batch 590 | Global Avg Train Loss: 3.5189\n",
      "Train | Epoch 0 | Batch 600 | Global Avg Train Loss: 3.5097\n",
      "Train | Epoch 0 | Batch 610 | Global Avg Train Loss: 3.5073\n",
      "Train | Epoch 0 | Batch 620 | Global Avg Train Loss: 3.4939\n",
      "Train | Epoch 0 | Batch 630 | Global Avg Train Loss: 3.4853\n",
      "Train | Epoch 0 | Batch 640 | Global Avg Train Loss: 3.4787\n",
      "Train | Epoch 0 | Batch 650 | Global Avg Train Loss: 3.4712\n",
      "Train | Epoch 0 | Batch 660 | Global Avg Train Loss: 3.4576\n",
      "Train | Epoch 0 | Batch 670 | Global Avg Train Loss: 3.4571\n",
      "Train | Epoch 0 | Batch 680 | Global Avg Train Loss: 3.4620\n",
      "Train | Epoch 0 | Batch 690 | Global Avg Train Loss: 3.4526\n",
      "Train | Epoch 0 | Batch 700 | Global Avg Train Loss: 3.4536\n",
      "Train | Epoch 0 | Batch 710 | Global Avg Train Loss: 3.4485\n",
      "Train | Epoch 0 | Batch 720 | Global Avg Train Loss: 3.4371\n",
      "Train | Epoch 0 | Batch 730 | Global Avg Train Loss: 3.4296\n",
      "Train | Epoch 0 | Batch 740 | Global Avg Train Loss: 3.4268\n",
      "Train | Epoch 0 | Batch 750 | Global Avg Train Loss: 3.4209\n",
      "Train | Epoch 0 | Batch 760 | Global Avg Train Loss: 3.4273\n",
      "Train | Epoch 0 | Batch 770 | Global Avg Train Loss: 3.4255\n",
      "Train | Epoch 0 | Batch 780 | Global Avg Train Loss: 3.4234\n",
      "Train | Epoch 0 | Batch 790 | Global Avg Train Loss: 3.4213\n",
      "Train | Epoch 0 | Batch 800 | Global Avg Train Loss: 3.4151\n",
      "Train | Epoch 0 | Batch 810 | Global Avg Train Loss: 3.4117\n",
      "Train | Epoch 0 | Batch 820 | Global Avg Train Loss: 3.4060\n",
      "Train | Epoch 0 | Batch 830 | Global Avg Train Loss: 3.4043\n",
      "Train | Epoch 0 | Batch 840 | Global Avg Train Loss: 3.4027\n",
      "Train | Epoch 0 | Batch 850 | Global Avg Train Loss: 3.3950\n",
      "Train | Epoch 0 | Batch 860 | Global Avg Train Loss: 3.3947\n",
      "Train | Epoch 0 | Batch 870 | Global Avg Train Loss: 3.3893\n",
      "Train | Epoch 0 | Batch 880 | Global Avg Train Loss: 3.3862\n",
      "Train | Epoch 0 | Batch 890 | Global Avg Train Loss: 3.3822\n",
      "Train | Epoch 0 | Batch 900 | Global Avg Train Loss: 3.3785\n",
      "Train | Epoch 0 | Batch 910 | Global Avg Train Loss: 3.3762\n",
      "Train | Epoch 0 | Batch 920 | Global Avg Train Loss: 3.3742\n",
      "Train | Epoch 0 | Batch 930 | Global Avg Train Loss: 3.3640\n",
      "Train | Epoch 0 | Batch 940 | Global Avg Train Loss: 3.3619\n",
      "Train | Epoch 0 | Batch 950 | Global Avg Train Loss: 3.3561\n",
      "Train | Epoch 0 | Batch 960 | Global Avg Train Loss: 3.3549\n",
      "Train | Epoch 0 | Batch 970 | Global Avg Train Loss: 3.3486\n",
      "Train | Epoch 0 | Batch 980 | Global Avg Train Loss: 3.3443\n",
      "Train | Epoch 0 | Batch 990 | Global Avg Train Loss: 3.3427\n",
      "Train | Epoch 0 | Batch 1000 | Global Avg Train Loss: 3.3393\n",
      "Train | Epoch 0 | Batch 1010 | Global Avg Train Loss: 3.3355\n",
      "Train | Epoch 0 | Batch 1020 | Global Avg Train Loss: 3.3326\n",
      "Train | Epoch 0 | Batch 1030 | Global Avg Train Loss: 3.3300\n",
      "Train | Epoch 0 | Batch 1040 | Global Avg Train Loss: 3.3284\n",
      "Train | Epoch 0 | Batch 1050 | Global Avg Train Loss: 3.3302\n",
      "Train | Epoch 0 | Batch 1060 | Global Avg Train Loss: 3.3278\n",
      "Train | Epoch 0 | Batch 1070 | Global Avg Train Loss: 3.3240\n",
      "Train | Epoch 0 | Batch 1080 | Global Avg Train Loss: 3.3256\n",
      "Train | Epoch 0 | Batch 1090 | Global Avg Train Loss: 3.3184\n",
      "Train | Epoch 0 | Batch 1100 | Global Avg Train Loss: 3.3136\n",
      "Train | Epoch 0 | Batch 1110 | Global Avg Train Loss: 3.3133\n",
      "Train | Epoch 0 | Batch 1120 | Global Avg Train Loss: 3.3109\n",
      "Train | Epoch 0 | Batch 1130 | Global Avg Train Loss: 3.3115\n",
      "Train | Epoch 0 | Batch 1140 | Global Avg Train Loss: 3.3090\n",
      "Train | Epoch 0 | Batch 1150 | Global Avg Train Loss: 3.3037\n",
      "Train | Epoch 0 | Batch 1160 | Global Avg Train Loss: 3.3097\n",
      "Train | Epoch 0 | Batch 1170 | Global Avg Train Loss: 3.3101\n",
      "Train | Epoch 0 | Batch 1180 | Global Avg Train Loss: 3.3128\n",
      "Train | Epoch 0 | Batch 1190 | Global Avg Train Loss: 3.3113\n",
      "Train | Epoch 0 | Batch 1200 | Global Avg Train Loss: 3.3075\n",
      "Train | Epoch 0 | Batch 1210 | Global Avg Train Loss: 3.3057\n",
      "Train | Epoch 0 | Batch 1220 | Global Avg Train Loss: 3.3032\n",
      "Train | Epoch 0 | Batch 1230 | Global Avg Train Loss: 3.3013\n",
      "Train | Epoch 0 | Batch 1240 | Global Avg Train Loss: 3.2981\n",
      "Train | Epoch 0 | Batch 1250 | Global Avg Train Loss: 3.2949\n",
      "Train | Epoch 0 | Batch 1260 | Global Avg Train Loss: 3.2939\n",
      "Train | Epoch 0 | Batch 1270 | Global Avg Train Loss: 3.2901\n",
      "Train | Epoch 0 | Batch 1280 | Global Avg Train Loss: 3.2911\n",
      "Train | Epoch 0 | Batch 1290 | Global Avg Train Loss: 3.2894\n",
      "Train | Epoch 0 | Batch 1300 | Global Avg Train Loss: 3.2916\n",
      "Train | Epoch 0 | Batch 1310 | Global Avg Train Loss: 3.2869\n",
      "Train | Epoch 0 | Batch 1320 | Global Avg Train Loss: 3.2836\n",
      "Train | Epoch 0 | Batch 1330 | Global Avg Train Loss: 3.2806\n",
      "Train | Epoch 0 | Batch 1340 | Global Avg Train Loss: 3.2765\n",
      "Train | Epoch 0 | Batch 1350 | Global Avg Train Loss: 3.2746\n",
      "Train | Epoch 0 | Batch 1360 | Global Avg Train Loss: 3.2734\n",
      "Train | Epoch 0 | Batch 1370 | Global Avg Train Loss: 3.2736\n",
      "Train | Epoch 0 | Batch 1380 | Global Avg Train Loss: 3.2723\n",
      "Train | Epoch 0 | Batch 1390 | Global Avg Train Loss: 3.2685\n",
      "Train | Epoch 0 | Batch 1400 | Global Avg Train Loss: 3.2657\n",
      "Train | Epoch 0 | Batch 1410 | Global Avg Train Loss: 3.2585\n",
      "Train | Epoch 0 | Batch 1420 | Global Avg Train Loss: 3.2555\n",
      "Train | Epoch 0 | Batch 1430 | Global Avg Train Loss: 3.2514\n",
      "Train | Epoch 0 | Batch 1440 | Global Avg Train Loss: 3.2495\n",
      "Epoch 0 finished | Global Avg Train Loss: 3.2465\n",
      "Epoch 1\n",
      "Train | Epoch 1 | Batch 0 | Global Avg Train Loss: 3.2460\n",
      "Train | Epoch 1 | Batch 10 | Global Avg Train Loss: 3.2408\n",
      "Train | Epoch 1 | Batch 20 | Global Avg Train Loss: 3.2395\n",
      "Train | Epoch 1 | Batch 30 | Global Avg Train Loss: 3.2366\n",
      "Train | Epoch 1 | Batch 40 | Global Avg Train Loss: 3.2322\n",
      "Train | Epoch 1 | Batch 50 | Global Avg Train Loss: 3.2270\n",
      "Train | Epoch 1 | Batch 60 | Global Avg Train Loss: 3.2240\n",
      "Train | Epoch 1 | Batch 70 | Global Avg Train Loss: 3.2197\n",
      "Train | Epoch 1 | Batch 80 | Global Avg Train Loss: 3.2202\n",
      "Train | Epoch 1 | Batch 90 | Global Avg Train Loss: 3.2193\n",
      "Train | Epoch 1 | Batch 100 | Global Avg Train Loss: 3.2184\n",
      "Train | Epoch 1 | Batch 110 | Global Avg Train Loss: 3.2150\n",
      "Train | Epoch 1 | Batch 120 | Global Avg Train Loss: 3.2137\n",
      "Train | Epoch 1 | Batch 130 | Global Avg Train Loss: 3.2124\n",
      "Train | Epoch 1 | Batch 140 | Global Avg Train Loss: 3.2100\n",
      "Train | Epoch 1 | Batch 150 | Global Avg Train Loss: 3.2089\n",
      "Train | Epoch 1 | Batch 160 | Global Avg Train Loss: 3.2043\n",
      "Train | Epoch 1 | Batch 170 | Global Avg Train Loss: 3.2013\n",
      "Train | Epoch 1 | Batch 180 | Global Avg Train Loss: 3.2014\n",
      "Train | Epoch 1 | Batch 190 | Global Avg Train Loss: 3.1988\n",
      "Train | Epoch 1 | Batch 200 | Global Avg Train Loss: 3.1962\n",
      "Train | Epoch 1 | Batch 210 | Global Avg Train Loss: 3.1934\n",
      "Train | Epoch 1 | Batch 220 | Global Avg Train Loss: 3.1907\n",
      "Train | Epoch 1 | Batch 230 | Global Avg Train Loss: 3.1903\n",
      "Train | Epoch 1 | Batch 240 | Global Avg Train Loss: 3.1890\n",
      "Train | Epoch 1 | Batch 250 | Global Avg Train Loss: 3.1831\n",
      "Train | Epoch 1 | Batch 260 | Global Avg Train Loss: 3.1784\n",
      "Train | Epoch 1 | Batch 270 | Global Avg Train Loss: 3.1756\n",
      "Train | Epoch 1 | Batch 280 | Global Avg Train Loss: 3.1760\n",
      "Train | Epoch 1 | Batch 290 | Global Avg Train Loss: 3.1742\n",
      "Train | Epoch 1 | Batch 300 | Global Avg Train Loss: 3.1738\n",
      "Train | Epoch 1 | Batch 310 | Global Avg Train Loss: 3.1731\n",
      "Train | Epoch 1 | Batch 320 | Global Avg Train Loss: 3.1693\n",
      "Train | Epoch 1 | Batch 330 | Global Avg Train Loss: 3.1697\n",
      "Train | Epoch 1 | Batch 340 | Global Avg Train Loss: 3.1650\n",
      "Train | Epoch 1 | Batch 350 | Global Avg Train Loss: 3.1616\n",
      "Train | Epoch 1 | Batch 360 | Global Avg Train Loss: 3.1611\n",
      "Train | Epoch 1 | Batch 370 | Global Avg Train Loss: 3.1594\n",
      "Train | Epoch 1 | Batch 380 | Global Avg Train Loss: 3.1592\n",
      "Train | Epoch 1 | Batch 390 | Global Avg Train Loss: 3.1572\n",
      "Train | Epoch 1 | Batch 400 | Global Avg Train Loss: 3.1574\n",
      "Train | Epoch 1 | Batch 410 | Global Avg Train Loss: 3.1570\n",
      "Train | Epoch 1 | Batch 420 | Global Avg Train Loss: 3.1547\n",
      "Train | Epoch 1 | Batch 430 | Global Avg Train Loss: 3.1525\n",
      "Train | Epoch 1 | Batch 440 | Global Avg Train Loss: 3.1518\n",
      "Train | Epoch 1 | Batch 450 | Global Avg Train Loss: 3.1503\n",
      "Train | Epoch 1 | Batch 460 | Global Avg Train Loss: 3.1486\n",
      "Train | Epoch 1 | Batch 470 | Global Avg Train Loss: 3.1467\n",
      "Train | Epoch 1 | Batch 480 | Global Avg Train Loss: 3.1425\n",
      "Train | Epoch 1 | Batch 490 | Global Avg Train Loss: 3.1379\n",
      "Train | Epoch 1 | Batch 500 | Global Avg Train Loss: 3.1351\n",
      "Train | Epoch 1 | Batch 510 | Global Avg Train Loss: 3.1348\n",
      "Train | Epoch 1 | Batch 520 | Global Avg Train Loss: 3.1323\n",
      "Train | Epoch 1 | Batch 530 | Global Avg Train Loss: 3.1315\n",
      "Train | Epoch 1 | Batch 540 | Global Avg Train Loss: 3.1285\n",
      "Train | Epoch 1 | Batch 550 | Global Avg Train Loss: 3.1251\n",
      "Train | Epoch 1 | Batch 560 | Global Avg Train Loss: 3.1238\n",
      "Train | Epoch 1 | Batch 570 | Global Avg Train Loss: 3.1239\n",
      "Train | Epoch 1 | Batch 580 | Global Avg Train Loss: 3.1233\n",
      "Train | Epoch 1 | Batch 590 | Global Avg Train Loss: 3.1204\n",
      "Train | Epoch 1 | Batch 600 | Global Avg Train Loss: 3.1175\n",
      "Train | Epoch 1 | Batch 610 | Global Avg Train Loss: 3.1154\n",
      "Train | Epoch 1 | Batch 620 | Global Avg Train Loss: 3.1131\n",
      "Train | Epoch 1 | Batch 630 | Global Avg Train Loss: 3.1122\n",
      "Train | Epoch 1 | Batch 640 | Global Avg Train Loss: 3.1091\n",
      "Train | Epoch 1 | Batch 650 | Global Avg Train Loss: 3.1070\n",
      "Train | Epoch 1 | Batch 660 | Global Avg Train Loss: 3.1049\n",
      "Train | Epoch 1 | Batch 670 | Global Avg Train Loss: 3.1037\n",
      "Train | Epoch 1 | Batch 680 | Global Avg Train Loss: 3.1018\n",
      "Train | Epoch 1 | Batch 690 | Global Avg Train Loss: 3.1004\n",
      "Train | Epoch 1 | Batch 700 | Global Avg Train Loss: 3.0975\n",
      "Train | Epoch 1 | Batch 710 | Global Avg Train Loss: 3.0950\n",
      "Train | Epoch 1 | Batch 720 | Global Avg Train Loss: 3.0941\n",
      "Train | Epoch 1 | Batch 730 | Global Avg Train Loss: 3.0920\n",
      "Train | Epoch 1 | Batch 740 | Global Avg Train Loss: 3.0891\n",
      "Train | Epoch 1 | Batch 750 | Global Avg Train Loss: 3.0872\n",
      "Train | Epoch 1 | Batch 760 | Global Avg Train Loss: 3.0866\n",
      "Train | Epoch 1 | Batch 770 | Global Avg Train Loss: 3.0850\n",
      "Train | Epoch 1 | Batch 780 | Global Avg Train Loss: 3.0833\n",
      "Train | Epoch 1 | Batch 790 | Global Avg Train Loss: 3.0815\n",
      "Train | Epoch 1 | Batch 800 | Global Avg Train Loss: 3.0815\n",
      "Train | Epoch 1 | Batch 810 | Global Avg Train Loss: 3.0817\n",
      "Train | Epoch 1 | Batch 820 | Global Avg Train Loss: 3.0825\n",
      "Train | Epoch 1 | Batch 830 | Global Avg Train Loss: 3.0809\n",
      "Train | Epoch 1 | Batch 840 | Global Avg Train Loss: 3.0786\n",
      "Train | Epoch 1 | Batch 850 | Global Avg Train Loss: 3.0793\n",
      "Train | Epoch 1 | Batch 860 | Global Avg Train Loss: 3.0801\n",
      "Train | Epoch 1 | Batch 870 | Global Avg Train Loss: 3.0804\n",
      "Train | Epoch 1 | Batch 880 | Global Avg Train Loss: 3.0764\n",
      "Train | Epoch 1 | Batch 890 | Global Avg Train Loss: 3.0776\n",
      "Train | Epoch 1 | Batch 900 | Global Avg Train Loss: 3.0762\n",
      "Train | Epoch 1 | Batch 910 | Global Avg Train Loss: 3.0736\n",
      "Train | Epoch 1 | Batch 920 | Global Avg Train Loss: 3.0706\n",
      "Train | Epoch 1 | Batch 930 | Global Avg Train Loss: 3.0698\n",
      "Train | Epoch 1 | Batch 940 | Global Avg Train Loss: 3.0708\n",
      "Train | Epoch 1 | Batch 950 | Global Avg Train Loss: 3.0699\n",
      "Train | Epoch 1 | Batch 960 | Global Avg Train Loss: 3.0677\n",
      "Train | Epoch 1 | Batch 970 | Global Avg Train Loss: 3.0667\n",
      "Train | Epoch 1 | Batch 980 | Global Avg Train Loss: 3.0666\n",
      "Train | Epoch 1 | Batch 990 | Global Avg Train Loss: 3.0636\n",
      "Train | Epoch 1 | Batch 1000 | Global Avg Train Loss: 3.0626\n",
      "Train | Epoch 1 | Batch 1010 | Global Avg Train Loss: 3.0608\n",
      "Train | Epoch 1 | Batch 1020 | Global Avg Train Loss: 3.0611\n",
      "Train | Epoch 1 | Batch 1030 | Global Avg Train Loss: 3.0590\n",
      "Train | Epoch 1 | Batch 1040 | Global Avg Train Loss: 3.0565\n",
      "Train | Epoch 1 | Batch 1050 | Global Avg Train Loss: 3.0555\n",
      "Train | Epoch 1 | Batch 1060 | Global Avg Train Loss: 3.0547\n",
      "Train | Epoch 1 | Batch 1070 | Global Avg Train Loss: 3.0558\n",
      "Train | Epoch 1 | Batch 1080 | Global Avg Train Loss: 3.0538\n",
      "Train | Epoch 1 | Batch 1090 | Global Avg Train Loss: 3.0531\n",
      "Train | Epoch 1 | Batch 1100 | Global Avg Train Loss: 3.0524\n",
      "Train | Epoch 1 | Batch 1110 | Global Avg Train Loss: 3.0526\n",
      "Train | Epoch 1 | Batch 1120 | Global Avg Train Loss: 3.0512\n",
      "Train | Epoch 1 | Batch 1130 | Global Avg Train Loss: 3.0519\n",
      "Train | Epoch 1 | Batch 1140 | Global Avg Train Loss: 3.0501\n",
      "Train | Epoch 1 | Batch 1150 | Global Avg Train Loss: 3.0505\n",
      "Train | Epoch 1 | Batch 1160 | Global Avg Train Loss: 3.0526\n",
      "Train | Epoch 1 | Batch 1170 | Global Avg Train Loss: 3.0509\n",
      "Train | Epoch 1 | Batch 1180 | Global Avg Train Loss: 3.0509\n",
      "Train | Epoch 1 | Batch 1190 | Global Avg Train Loss: 3.0493\n",
      "Train | Epoch 1 | Batch 1200 | Global Avg Train Loss: 3.0468\n",
      "Train | Epoch 1 | Batch 1210 | Global Avg Train Loss: 3.0460\n",
      "Train | Epoch 1 | Batch 1220 | Global Avg Train Loss: 3.0456\n",
      "Train | Epoch 1 | Batch 1230 | Global Avg Train Loss: 3.0448\n",
      "Train | Epoch 1 | Batch 1240 | Global Avg Train Loss: 3.0450\n",
      "Train | Epoch 1 | Batch 1250 | Global Avg Train Loss: 3.0422\n",
      "Train | Epoch 1 | Batch 1260 | Global Avg Train Loss: 3.0410\n",
      "Train | Epoch 1 | Batch 1270 | Global Avg Train Loss: 3.0397\n",
      "Train | Epoch 1 | Batch 1280 | Global Avg Train Loss: 3.0390\n",
      "Train | Epoch 1 | Batch 1290 | Global Avg Train Loss: 3.0388\n",
      "Train | Epoch 1 | Batch 1300 | Global Avg Train Loss: 3.0395\n",
      "Train | Epoch 1 | Batch 1310 | Global Avg Train Loss: 3.0391\n",
      "Train | Epoch 1 | Batch 1320 | Global Avg Train Loss: 3.0368\n",
      "Train | Epoch 1 | Batch 1330 | Global Avg Train Loss: 3.0357\n",
      "Train | Epoch 1 | Batch 1340 | Global Avg Train Loss: 3.0350\n",
      "Train | Epoch 1 | Batch 1350 | Global Avg Train Loss: 3.0330\n",
      "Train | Epoch 1 | Batch 1360 | Global Avg Train Loss: 3.0322\n",
      "Train | Epoch 1 | Batch 1370 | Global Avg Train Loss: 3.0327\n",
      "Train | Epoch 1 | Batch 1380 | Global Avg Train Loss: 3.0320\n",
      "Train | Epoch 1 | Batch 1390 | Global Avg Train Loss: 3.0296\n",
      "Train | Epoch 1 | Batch 1400 | Global Avg Train Loss: 3.0293\n",
      "Train | Epoch 1 | Batch 1410 | Global Avg Train Loss: 3.0294\n",
      "Train | Epoch 1 | Batch 1420 | Global Avg Train Loss: 3.0310\n",
      "Train | Epoch 1 | Batch 1430 | Global Avg Train Loss: 3.0297\n",
      "Train | Epoch 1 | Batch 1440 | Global Avg Train Loss: 3.0286\n",
      "Epoch 1 finished | Global Avg Train Loss: 3.0288\n",
      "Epoch 2\n",
      "Train | Epoch 2 | Batch 0 | Global Avg Train Loss: 3.0287\n",
      "Train | Epoch 2 | Batch 10 | Global Avg Train Loss: 3.0259\n",
      "Train | Epoch 2 | Batch 20 | Global Avg Train Loss: 3.0236\n",
      "Train | Epoch 2 | Batch 30 | Global Avg Train Loss: 3.0216\n",
      "Train | Epoch 2 | Batch 40 | Global Avg Train Loss: 3.0223\n",
      "Train | Epoch 2 | Batch 50 | Global Avg Train Loss: 3.0213\n",
      "Train | Epoch 2 | Batch 60 | Global Avg Train Loss: 3.0205\n",
      "Train | Epoch 2 | Batch 70 | Global Avg Train Loss: 3.0201\n",
      "Train | Epoch 2 | Batch 80 | Global Avg Train Loss: 3.0185\n",
      "Train | Epoch 2 | Batch 90 | Global Avg Train Loss: 3.0171\n",
      "Train | Epoch 2 | Batch 100 | Global Avg Train Loss: 3.0158\n",
      "Train | Epoch 2 | Batch 110 | Global Avg Train Loss: 3.0155\n",
      "Train | Epoch 2 | Batch 120 | Global Avg Train Loss: 3.0135\n",
      "Train | Epoch 2 | Batch 130 | Global Avg Train Loss: 3.0108\n",
      "Train | Epoch 2 | Batch 140 | Global Avg Train Loss: 3.0090\n",
      "Train | Epoch 2 | Batch 150 | Global Avg Train Loss: 3.0074\n",
      "Train | Epoch 2 | Batch 160 | Global Avg Train Loss: 3.0058\n",
      "Train | Epoch 2 | Batch 170 | Global Avg Train Loss: 3.0052\n",
      "Train | Epoch 2 | Batch 180 | Global Avg Train Loss: 3.0050\n",
      "Train | Epoch 2 | Batch 190 | Global Avg Train Loss: 3.0046\n",
      "Train | Epoch 2 | Batch 200 | Global Avg Train Loss: 3.0033\n",
      "Train | Epoch 2 | Batch 210 | Global Avg Train Loss: 3.0012\n",
      "Train | Epoch 2 | Batch 220 | Global Avg Train Loss: 2.9992\n",
      "Train | Epoch 2 | Batch 230 | Global Avg Train Loss: 2.9970\n",
      "Train | Epoch 2 | Batch 240 | Global Avg Train Loss: 2.9946\n",
      "Train | Epoch 2 | Batch 250 | Global Avg Train Loss: 2.9949\n",
      "Train | Epoch 2 | Batch 260 | Global Avg Train Loss: 2.9954\n",
      "Train | Epoch 2 | Batch 270 | Global Avg Train Loss: 2.9956\n",
      "Train | Epoch 2 | Batch 280 | Global Avg Train Loss: 2.9936\n",
      "Train | Epoch 2 | Batch 290 | Global Avg Train Loss: 2.9926\n",
      "Train | Epoch 2 | Batch 300 | Global Avg Train Loss: 2.9922\n",
      "Train | Epoch 2 | Batch 310 | Global Avg Train Loss: 2.9900\n",
      "Train | Epoch 2 | Batch 320 | Global Avg Train Loss: 2.9892\n",
      "Train | Epoch 2 | Batch 330 | Global Avg Train Loss: 2.9873\n",
      "Train | Epoch 2 | Batch 340 | Global Avg Train Loss: 2.9874\n",
      "Train | Epoch 2 | Batch 350 | Global Avg Train Loss: 2.9865\n",
      "Train | Epoch 2 | Batch 360 | Global Avg Train Loss: 2.9850\n",
      "Train | Epoch 2 | Batch 370 | Global Avg Train Loss: 2.9837\n",
      "Train | Epoch 2 | Batch 380 | Global Avg Train Loss: 2.9819\n",
      "Train | Epoch 2 | Batch 390 | Global Avg Train Loss: 2.9812\n",
      "Train | Epoch 2 | Batch 400 | Global Avg Train Loss: 2.9792\n",
      "Train | Epoch 2 | Batch 410 | Global Avg Train Loss: 2.9784\n",
      "Train | Epoch 2 | Batch 420 | Global Avg Train Loss: 2.9777\n",
      "Train | Epoch 2 | Batch 430 | Global Avg Train Loss: 2.9752\n",
      "Train | Epoch 2 | Batch 440 | Global Avg Train Loss: 2.9738\n",
      "Train | Epoch 2 | Batch 450 | Global Avg Train Loss: 2.9723\n",
      "Train | Epoch 2 | Batch 460 | Global Avg Train Loss: 2.9722\n",
      "Train | Epoch 2 | Batch 470 | Global Avg Train Loss: 2.9712\n",
      "Train | Epoch 2 | Batch 480 | Global Avg Train Loss: 2.9712\n",
      "Train | Epoch 2 | Batch 490 | Global Avg Train Loss: 2.9698\n",
      "Train | Epoch 2 | Batch 500 | Global Avg Train Loss: 2.9679\n",
      "Train | Epoch 2 | Batch 510 | Global Avg Train Loss: 2.9656\n",
      "Train | Epoch 2 | Batch 520 | Global Avg Train Loss: 2.9644\n",
      "Train | Epoch 2 | Batch 530 | Global Avg Train Loss: 2.9632\n",
      "Train | Epoch 2 | Batch 540 | Global Avg Train Loss: 2.9630\n",
      "Train | Epoch 2 | Batch 550 | Global Avg Train Loss: 2.9623\n",
      "Train | Epoch 2 | Batch 560 | Global Avg Train Loss: 2.9614\n",
      "Train | Epoch 2 | Batch 570 | Global Avg Train Loss: 2.9601\n",
      "Train | Epoch 2 | Batch 580 | Global Avg Train Loss: 2.9584\n",
      "Train | Epoch 2 | Batch 590 | Global Avg Train Loss: 2.9583\n",
      "Train | Epoch 2 | Batch 600 | Global Avg Train Loss: 2.9569\n",
      "Train | Epoch 2 | Batch 610 | Global Avg Train Loss: 2.9563\n",
      "Train | Epoch 2 | Batch 620 | Global Avg Train Loss: 2.9558\n",
      "Train | Epoch 2 | Batch 630 | Global Avg Train Loss: 2.9570\n",
      "Train | Epoch 2 | Batch 640 | Global Avg Train Loss: 2.9556\n",
      "Train | Epoch 2 | Batch 650 | Global Avg Train Loss: 2.9557\n",
      "Train | Epoch 2 | Batch 660 | Global Avg Train Loss: 2.9557\n",
      "Train | Epoch 2 | Batch 670 | Global Avg Train Loss: 2.9541\n",
      "Train | Epoch 2 | Batch 680 | Global Avg Train Loss: 2.9524\n",
      "Train | Epoch 2 | Batch 690 | Global Avg Train Loss: 2.9506\n",
      "Train | Epoch 2 | Batch 700 | Global Avg Train Loss: 2.9492\n",
      "Train | Epoch 2 | Batch 710 | Global Avg Train Loss: 2.9485\n",
      "Train | Epoch 2 | Batch 720 | Global Avg Train Loss: 2.9483\n",
      "Train | Epoch 2 | Batch 730 | Global Avg Train Loss: 2.9481\n",
      "Train | Epoch 2 | Batch 740 | Global Avg Train Loss: 2.9474\n",
      "Train | Epoch 2 | Batch 750 | Global Avg Train Loss: 2.9459\n",
      "Train | Epoch 2 | Batch 760 | Global Avg Train Loss: 2.9451\n",
      "Train | Epoch 2 | Batch 770 | Global Avg Train Loss: 2.9444\n",
      "Train | Epoch 2 | Batch 780 | Global Avg Train Loss: 2.9434\n",
      "Train | Epoch 2 | Batch 790 | Global Avg Train Loss: 2.9426\n",
      "Train | Epoch 2 | Batch 800 | Global Avg Train Loss: 2.9414\n",
      "Train | Epoch 2 | Batch 810 | Global Avg Train Loss: 2.9415\n",
      "Train | Epoch 2 | Batch 820 | Global Avg Train Loss: 2.9403\n",
      "Train | Epoch 2 | Batch 830 | Global Avg Train Loss: 2.9393\n",
      "Train | Epoch 2 | Batch 840 | Global Avg Train Loss: 2.9389\n",
      "Train | Epoch 2 | Batch 850 | Global Avg Train Loss: 2.9383\n",
      "Train | Epoch 2 | Batch 860 | Global Avg Train Loss: 2.9372\n",
      "Train | Epoch 2 | Batch 870 | Global Avg Train Loss: 2.9365\n",
      "Train | Epoch 2 | Batch 880 | Global Avg Train Loss: 2.9357\n",
      "Train | Epoch 2 | Batch 890 | Global Avg Train Loss: 2.9344\n",
      "Train | Epoch 2 | Batch 900 | Global Avg Train Loss: 2.9339\n",
      "Train | Epoch 2 | Batch 910 | Global Avg Train Loss: 2.9327\n",
      "Train | Epoch 2 | Batch 920 | Global Avg Train Loss: 2.9325\n",
      "Train | Epoch 2 | Batch 930 | Global Avg Train Loss: 2.9329\n",
      "Train | Epoch 2 | Batch 940 | Global Avg Train Loss: 2.9335\n",
      "Train | Epoch 2 | Batch 950 | Global Avg Train Loss: 2.9328\n",
      "Train | Epoch 2 | Batch 960 | Global Avg Train Loss: 2.9331\n",
      "Train | Epoch 2 | Batch 970 | Global Avg Train Loss: 2.9319\n",
      "Train | Epoch 2 | Batch 980 | Global Avg Train Loss: 2.9299\n",
      "Train | Epoch 2 | Batch 990 | Global Avg Train Loss: 2.9290\n",
      "Train | Epoch 2 | Batch 1000 | Global Avg Train Loss: 2.9276\n",
      "Train | Epoch 2 | Batch 1010 | Global Avg Train Loss: 2.9269\n",
      "Train | Epoch 2 | Batch 1020 | Global Avg Train Loss: 2.9274\n",
      "Train | Epoch 2 | Batch 1030 | Global Avg Train Loss: 2.9261\n",
      "Train | Epoch 2 | Batch 1040 | Global Avg Train Loss: 2.9256\n",
      "Train | Epoch 2 | Batch 1050 | Global Avg Train Loss: 2.9260\n",
      "Train | Epoch 2 | Batch 1060 | Global Avg Train Loss: 2.9256\n",
      "Train | Epoch 2 | Batch 1070 | Global Avg Train Loss: 2.9256\n",
      "Train | Epoch 2 | Batch 1080 | Global Avg Train Loss: 2.9237\n",
      "Train | Epoch 2 | Batch 1090 | Global Avg Train Loss: 2.9241\n",
      "Train | Epoch 2 | Batch 1100 | Global Avg Train Loss: 2.9224\n",
      "Train | Epoch 2 | Batch 1110 | Global Avg Train Loss: 2.9212\n",
      "Train | Epoch 2 | Batch 1120 | Global Avg Train Loss: 2.9200\n",
      "Train | Epoch 2 | Batch 1130 | Global Avg Train Loss: 2.9194\n",
      "Train | Epoch 2 | Batch 1140 | Global Avg Train Loss: 2.9184\n",
      "Train | Epoch 2 | Batch 1150 | Global Avg Train Loss: 2.9180\n",
      "Train | Epoch 2 | Batch 1160 | Global Avg Train Loss: 2.9174\n",
      "Train | Epoch 2 | Batch 1170 | Global Avg Train Loss: 2.9171\n",
      "Train | Epoch 2 | Batch 1180 | Global Avg Train Loss: 2.9175\n",
      "Train | Epoch 2 | Batch 1190 | Global Avg Train Loss: 2.9182\n",
      "Train | Epoch 2 | Batch 1200 | Global Avg Train Loss: 2.9180\n",
      "Train | Epoch 2 | Batch 1210 | Global Avg Train Loss: 2.9175\n",
      "Train | Epoch 2 | Batch 1220 | Global Avg Train Loss: 2.9171\n",
      "Train | Epoch 2 | Batch 1230 | Global Avg Train Loss: 2.9157\n",
      "Train | Epoch 2 | Batch 1240 | Global Avg Train Loss: 2.9155\n",
      "Train | Epoch 2 | Batch 1250 | Global Avg Train Loss: 2.9156\n",
      "Train | Epoch 2 | Batch 1260 | Global Avg Train Loss: 2.9142\n",
      "Train | Epoch 2 | Batch 1270 | Global Avg Train Loss: 2.9139\n",
      "Train | Epoch 2 | Batch 1280 | Global Avg Train Loss: 2.9129\n",
      "Train | Epoch 2 | Batch 1290 | Global Avg Train Loss: 2.9136\n",
      "Train | Epoch 2 | Batch 1300 | Global Avg Train Loss: 2.9128\n",
      "Train | Epoch 2 | Batch 1310 | Global Avg Train Loss: 2.9116\n",
      "Train | Epoch 2 | Batch 1320 | Global Avg Train Loss: 2.9105\n",
      "Train | Epoch 2 | Batch 1330 | Global Avg Train Loss: 2.9106\n",
      "Train | Epoch 2 | Batch 1340 | Global Avg Train Loss: 2.9109\n",
      "Train | Epoch 2 | Batch 1350 | Global Avg Train Loss: 2.9112\n",
      "Train | Epoch 2 | Batch 1360 | Global Avg Train Loss: 2.9103\n",
      "Train | Epoch 2 | Batch 1370 | Global Avg Train Loss: 2.9089\n",
      "Train | Epoch 2 | Batch 1380 | Global Avg Train Loss: 2.9076\n",
      "Train | Epoch 2 | Batch 1390 | Global Avg Train Loss: 2.9066\n",
      "Train | Epoch 2 | Batch 1400 | Global Avg Train Loss: 2.9066\n",
      "Train | Epoch 2 | Batch 1410 | Global Avg Train Loss: 2.9062\n",
      "Train | Epoch 2 | Batch 1420 | Global Avg Train Loss: 2.9059\n",
      "Train | Epoch 2 | Batch 1430 | Global Avg Train Loss: 2.9057\n",
      "Train | Epoch 2 | Batch 1440 | Global Avg Train Loss: 2.9054\n",
      "Epoch 2 finished | Global Avg Train Loss: 2.9045\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure padding has a pad token\n",
    "\n",
    "model_config = GPT2Config(\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_hidden_layers=NUM_HIDDEN_LAYERS,\n",
    "    num_attention_heads=NUM_ATTENTION_HEADS,\n",
    "    intermediate_size=HIDDEN_SIZE*3,\n",
    ")\n",
    "\n",
    "toy_gpt2_model = GPT2Model(model_config).to(DEVICE)\n",
    "\n",
    "VOCAB_SIZE = model_config.vocab_size\n",
    "\n",
    "dataset = TextDataset('pretrain.txt', tokenizer, MAX_SEQ_LEN)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(toy_gpt2_model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, correct_bias=CORRECT_BIAS)\n",
    "\n",
    "\n",
    "global_train_losses = []\n",
    "\n",
    "total_train_loss = 0.0\n",
    "total_train_steps = 0\n",
    "\n",
    "\n",
    "print_interval = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    toy_gpt2_model.train()\n",
    "    for batch_idx, (input_ids, attention_mask) in enumerate(dataloader):\n",
    "        input_ids = input_ids.to(DEVICE)\n",
    "        attention_mask = attention_mask.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hidden_states = toy_gpt2_model(input_ids, attention_mask)['last_hidden_state']  # [B, seq_len, hidden]\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO-7: Compute next-token loss from hidden states and update model parameters.\n",
    "\n",
    "        Implementation hints:\n",
    "        1. Convert hidden states to logits over the vocabulary using model.hidden_state_to_token.\n",
    "        2. Shift logits and labels for next-token prediction to align each prediction with the correct next token.\n",
    "        3. Compute the cross-entropy loss.\n",
    "        4. Backpropagate and update parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        # Convert hidden states to logits over the vocabulary using model.hidden_state_to_token.\n",
    "        logits = toy_gpt2_model.hidden_state_to_token(hidden_states)\n",
    "        # Shift logits and labels for next-token prediction to align each prediction with the correct next token.\n",
    "        logits = logits[:, :-1, :]\n",
    "        labels = input_ids[:, 1:]\n",
    "        # Compute the cross-entropy loss.\n",
    "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), labels.reshape(-1))\n",
    "        # Backpropagate and update parameters.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # raise NotImplementedError\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        total_train_steps += 1\n",
    "        global_train_avg_loss = total_train_loss / total_train_steps\n",
    "        global_train_losses.append(global_train_avg_loss)\n",
    "\n",
    "        if batch_idx % print_interval == 0:\n",
    "            print(f\"Train | Epoch {epoch} | Batch {batch_idx} | Global Avg Train Loss: {global_train_avg_loss:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch} finished | Global Avg Train Loss: {global_train_avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1680f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sanity check: decreasing trend of global average training loss\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(global_train_losses, label=\"Global Avg Train Loss\", color='blue')\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Global Cumulative Avg Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1a3459",
   "metadata": {},
   "source": [
    "# Task 2: English NLI with GPT-2\n",
    "In this task, you will:\n",
    "- Load a pretrained GPT-2 model with official weights and perform a dummy text generation.\n",
    "- Load an English Natural Language Inference (NLI) dataset.\n",
    "- Fine-tune the loaded model and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b249d1f0",
   "metadata": {},
   "source": [
    "## Model Loading & Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc299457",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_gpt2(model, tokenizer, input_ids, max_gen_length=50, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Generate text from a GPT-2 model given a single input sequence (greedy decoding).\n",
    "\n",
    "    Note:\n",
    "        - Currently only supports batch_size=1 (single input sequence).\n",
    "        - Using greedy decoding, so each run with the same input produces the same output.\n",
    "        - Other sampling-based decoding methods (e.g., top-k, top-p, temperature) can introduce randomness and yield different outputs each run.\n",
    "\n",
    "    Args:\n",
    "        model: GPT-2 model (pretrained or fine-tuned)\n",
    "        tokenizer: GPT-2 tokenizer\n",
    "        input_ids: torch.LongTensor of shape [1, seq_len], input token IDs\n",
    "        max_gen_length: int, maximum number of tokens to generate\n",
    "        device: str, \"cuda\" or \"cpu\"\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_ids = input_ids.to(device)  # move input to device\n",
    "    output_ids = input_ids.clone()\n",
    "\n",
    "    \"\"\"\n",
    "    TODO-8: Greedy next-token generation loop\n",
    "\n",
    "    Implementation hints:\n",
    "    Repeat the below steps up to max_gen_length:\n",
    "    1. Construct an attention mask based on current output_ids (non-pad tokens).\n",
    "    2. Pass output_ids and attention_mask through the model to get hidden states.\n",
    "    3. Convert the last hidden state to logits over the vocabulary using model.hidden_state_to_token.\n",
    "    4. Select the next token using greedy decoding (argmax over logits).\n",
    "    5. Append the next token to output_ids.\n",
    "    6. Stop the loop early if the EOS token is generated.\n",
    "    \n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    # Store initial length to track how many NEW tokens to generate\n",
    "    # Bug fix: max_gen_length is the number of NEW tokens, not total length\n",
    "    initial_length = len(output_ids[0])\n",
    "    while len(output_ids[0]) < initial_length + max_gen_length: \n",
    "\n",
    "        # Construct an attention mask based on current output_ids (non-pad tokens).\n",
    "        attention_mask = torch.ones_like(output_ids)\n",
    "\n",
    "        # Pass output_ids and attention_mask through the model to get hidden states.\n",
    "        hidden_states = model(output_ids, attention_mask)['last_hidden_state']\n",
    "\n",
    "        # Convert the last hidden state to logits over the vocabulary using model.hidden_state_to_token.\n",
    "        logits = model.hidden_state_to_token(hidden_states)\n",
    "\n",
    "        # Select the next token using greedy decoding (argmax over logits).\n",
    "        last_token = logits[:, -1, :]   # [batch_size, vocab_size]\n",
    "        next_token = torch.argmax(last_token, dim=1)\n",
    "\n",
    "        # Append the next token to output_ids.\n",
    "        next_token = next_token.unsqueeze(1)    # add sequence dimension\n",
    "        output_ids = torch.cat([output_ids, next_token], dim=1)\n",
    "\n",
    "        # Stop the loop early if the EOS token is generated.\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    # raise NotImplementedError\n",
    "\n",
    "    # Decode generated tokens to string\n",
    "    ids = output_ids[0]\n",
    "    text = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7cc58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained GPT-2 model with official weights\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2Model.from_pretrained(\"gpt2\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d2e14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singapore University of Technology and Design (SUTD) is a leading research institute in the field of design and engineering. It is the first university in Singapore to offer a full-time, full-time, full-time, full-time\n"
     ]
    }
   ],
   "source": [
    "# Dummy text generation using the pretrained GPT-2 model with official weights\n",
    "dummy_texts = \"Singapore University of Technology and Design (SUTD) is\"\n",
    "input_ids = tokenizer(dummy_texts, return_tensors=\"pt\", padding=True)['input_ids']\n",
    "generated_texts = generate_gpt2(model, tokenizer, input_ids, max_gen_length=50, device=DEVICE)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f40935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singapore University of Technology and Design (SUTD) is a $3.\n"
     ]
    }
   ],
   "source": [
    "# Dummy text generation using the toy GPT-2 model trained in Task 1\n",
    "generated_texts = generate_gpt2(toy_gpt2_model, tokenizer, input_ids, max_gen_length=50, device=DEVICE)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b611373",
   "metadata": {},
   "source": [
    "## Load NLI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f25161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, labels):\n",
    "    correct = sum(p.lower().strip() == l.lower().strip() for p, l in zip(preds, labels))\n",
    "    return correct / len(labels)\n",
    "\n",
    "def evaluate_gpt2_xnli(model, tokenizer, dataloader, max_gen_length=10, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(dataloader, desc=\"Generating\"):\n",
    "            input_ids = item['input_ids']\n",
    "            gen_text = generate_gpt2(model, tokenizer, input_ids, max_gen_length=max_gen_length, device=device)\n",
    "            pred_label = gen_text.split(\"Label:\")[-1].strip()\n",
    "            all_preds.append(pred_label)\n",
    "            all_labels.extend(item['label_strs'])\n",
    "    acc = compute_accuracy(all_preds, all_labels)\n",
    "    print(f\"Evaluation accuracy: {acc*100:.2f}%\")\n",
    "    return acc, all_preds, all_labels\n",
    "\n",
    "class XNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for XNLI (Cross-lingual Natural Language Inference) task.\n",
    "\n",
    "    Supports train, dev, and test splits in a specific language, \n",
    "    tokenizes text inputs for GPT-style models, and optionally subsamples the dataset.\n",
    "\n",
    "    Attributes:\n",
    "        split (str): Dataset split, one of 'train', 'dev', 'test'.\n",
    "        lang (str): Language code (e.g., 'en', 'zh').\n",
    "        tokenizer: A HuggingFace tokenizer to convert text to input IDs.\n",
    "        max_length (int): Maximum sequence length for tokenization.\n",
    "        LABEL2ID (dict): Mapping from textual labels to integer IDs.\n",
    "        ID2LABEL (dict): Reverse mapping from integer IDs to textual labels.\n",
    "        data (pd.DataFrame): The loaded and preprocessed dataset.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        split=\"train\",\n",
    "        lang=\"en\",\n",
    "        train_path_template=\"XNLI-MT-1.0/multinli/multinli.train.{lang}.tsv\",\n",
    "        test_path=\"XNLI-1.0/xnli.test.tsv\",\n",
    "        dev_path=\"XNLI-1.0/xnli.dev.tsv\",\n",
    "        tokenizer=None,\n",
    "        max_length=1024,\n",
    "        subset = 1.0  # 0~1\n",
    "    ):\n",
    "        self.split = split\n",
    "        self.lang = lang\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.LABEL2ID = {\"entailment\": 0, \"contradictory\": 1, \"neutral\": 2}\n",
    "        self.ID2LABEL = {v: k for k, v in self.LABEL2ID.items()}\n",
    "\n",
    "        if split == \"train\":\n",
    "            path = train_path_template.format(lang=lang)\n",
    "            df = self.read_xnli_tsv(path, split)\n",
    "            df = df.dropna(subset=['premise','hypo','label'])\n",
    "        elif split in [\"dev\", \"test\"]:\n",
    "            path = test_path if split==\"test\" else dev_path\n",
    "            df = self.read_xnli_tsv(path, split)\n",
    "            df = df[df['language']==lang].copy()\n",
    "            keep_cols = ['sentence1', 'sentence2', 'gold_label']\n",
    "            df = df[keep_cols].dropna()\n",
    "            df.rename(columns={'sentence1':'premise','sentence2':'hypo','gold_label':'label'}, inplace=True)\n",
    "            df['label'] = df['label'].replace({'contradiction': 'contradictory'})\n",
    "        else:\n",
    "            raise ValueError(\"split must be one of ['train','dev','test']\")\n",
    "        \n",
    "        original_num = len(df)\n",
    "        if subset < 1.0:\n",
    "            n = max(1, int(len(df) * subset))\n",
    "            df = df.iloc[:n].reset_index(drop=True)\n",
    "        subset_num = len(df)\n",
    "\n",
    "        self.data = df.reset_index(drop=True)\n",
    "        print(f\"Dataset initialized: split='{split}', lang='{lang}', total={original_num}, subset={subset}, subset_count={subset_num}\")\n",
    "\n",
    "    def read_xnli_tsv(self, path, split):\n",
    "        \"\"\"\n",
    "        Read an XNLI TSV file and return it as a pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            path (str): Path to the TSV file.\n",
    "            split (str): One of \"train\", \"dev\", \"test\" indicating the dataset split.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The dataset as a DataFrame with appropriate columns.\n",
    "        \"\"\"\n",
    "        if split == \"train\":\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines()\n",
    "            header = lines[0].split(\"\\t\")\n",
    "            data = []\n",
    "            for i, line in enumerate(lines[1:], start=2):\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts) == len(header):\n",
    "                    data.append(parts)\n",
    "                else:\n",
    "                    print(f\"skip row {i}: {len(parts)} cols  {parts[:2]}\")\n",
    "        else:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                reader = csv.reader(f, delimiter=\"\\t\")\n",
    "                rows = list(reader)\n",
    "            header = rows[0]\n",
    "            expected_cols = len(header)\n",
    "            data = []\n",
    "            for i, row in enumerate(rows[1:], start=2):\n",
    "                if len(row) == expected_cols:\n",
    "                    data.append(row)\n",
    "                else:\n",
    "                    print(f\"skip row {i}: {len(row)} cols  {row[:2]}\")\n",
    "        return pd.DataFrame(data, columns=header)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of examples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a single example by index and tokenize it.\n",
    "\n",
    "        For training split:\n",
    "            - Constructs the input as \"Premise: ... Hypothesis: ... Label: ...\"\n",
    "            - Tokenizes the full input.\n",
    "            - Masks the prefix tokens in the labels with -100 for GPT loss computation.\n",
    "\n",
    "        For dev/test split:\n",
    "            - Constructs the input without label as \"Premise: ... Hypothesis: ... Label:\"\n",
    "\n",
    "        Returns:\n",
    "            dict: Contains 'input_ids', 'attention_mask', 'labels' (train only), 'label_str'\n",
    "        \"\"\"\n",
    "        row = self.data.iloc[idx]\n",
    "        premise = row['premise']\n",
    "        hypo = row['hypo']\n",
    "        label = row['label']\n",
    "        if self.lang == 'zh': # de-tokenize for Chinese\n",
    "            premise = premise.replace(\" \", \"\")\n",
    "            hypo = hypo.replace(\" \", \"\")\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            prefix = f\"Premise: {premise}\\nHypothesis: {hypo}\\nLabel:\"\n",
    "            full_text = prefix + str(self.LABEL2ID[label])\n",
    "            tokenized = self.tokenizer(\n",
    "                full_text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=False,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tokenized = {k: v.squeeze(0) for k, v in tokenized.items()}\n",
    "\n",
    "            prefix_ids = self.tokenizer(prefix).input_ids\n",
    "            labels_ids = tokenized['input_ids'].clone()\n",
    "            labels_ids[:len(prefix_ids)] = -100 # Masks the prefix tokens in the labels with -100 for GPT loss computation.\n",
    "            tokenized['labels'] = labels_ids\n",
    "            tokenized['label_str'] = str(self.LABEL2ID[label])\n",
    "            return tokenized\n",
    "        else:\n",
    "            text = f\"Premise: {premise}\\nHypothesis: {hypo}\\nLabel:\"\n",
    "            tokenized = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=False,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tokenized = {k: v.squeeze(0) for k, v in tokenized.items()}\n",
    "            tokenized['label_str'] = str(self.LABEL2ID[label])\n",
    "            return tokenized\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Collate a batch of examples into padded tensors.\n",
    "\n",
    "        Pads 'input_ids' and 'attention_mask' to the max length in the batch.\n",
    "        Pads 'labels' with -100 if present.\n",
    "        Collects 'label_str' for reference.\n",
    "\n",
    "        Returns:\n",
    "            dict: Padded tensors and label strings for the batch.\n",
    "        \"\"\"\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            [b['input_ids'] for b in batch],\n",
    "            batch_first=True,\n",
    "            padding_value=0\n",
    "        )\n",
    "        attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "            [b['attention_mask'] for b in batch],\n",
    "            batch_first=True,\n",
    "            padding_value=0\n",
    "        )\n",
    "\n",
    "        if 'labels' in batch[0]:\n",
    "            labels = torch.nn.utils.rnn.pad_sequence(\n",
    "                [b['labels'] for b in batch],\n",
    "                batch_first=True,\n",
    "                padding_value=-100\n",
    "            )\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        label_strs = [b['label_str'] for b in batch]\n",
    "\n",
    "        out = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label_strs\": label_strs}\n",
    "        if labels is not None:\n",
    "            out[\"labels\"] = labels\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa19d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized: split='train', lang='en', total=392702, subset=1, subset_count=392702\n",
      "Dataset initialized: split='dev', lang='en', total=2490, subset=1, subset_count=2490\n",
      "Dataset initialized: split='test', lang='en', total=5010, subset=1, subset_count=5010\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load NLI datasets for fine-tuning and evaluation.\n",
    "For debugging on a CPU, you can set SUBSET to a float in (0,1) to load only a fraction of the data.\n",
    "Final training and evaluation should use the full dataset (SUBSET=1).\n",
    "\"\"\"\n",
    "\n",
    "TRAIN_SUBSET = 1\n",
    "DEV_SUBSET = 1\n",
    "TEST_SUBSET = 1\n",
    "\n",
    "train_dataset = XNLIDataset(\n",
    "    split=\"train\",\n",
    "    lang=\"en\",\n",
    "    tokenizer=tokenizer,\n",
    "    subset=TRAIN_SUBSET\n",
    ")\n",
    "\n",
    "dev_dataset = XNLIDataset(\n",
    "    split=\"dev\",\n",
    "    lang=\"en\",\n",
    "    tokenizer=tokenizer,\n",
    "    subset=DEV_SUBSET\n",
    ")\n",
    "\n",
    "test_dataset = XNLIDataset(\n",
    "    split=\"test\",\n",
    "    lang=\"en\",\n",
    "    tokenizer=tokenizer,\n",
    "    subset=TEST_SUBSET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3196cd4",
   "metadata": {},
   "source": [
    "## Fine-tune GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamter of gpt2 fine-tuning\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 4\n",
    "LR = 5e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "CORRECT_BIAS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8142b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 98176/98176 [1:19:44<00:00, 20.52it/s, avg_loss=0.6154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished | Global Avg Loss: 0.6154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|| 2490/2490 [00:24<00:00, 102.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 79.40%\n",
      "New best model saved at best_model/model.pt with dev accuracy 79.40%\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders for training and validation datasets\n",
    "train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,collate_fn=XNLIDataset.collate_fn)\n",
    "dev_loader = DataLoader(dev_dataset,shuffle=False,collate_fn=XNLIDataset.collate_fn)\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, correct_bias=CORRECT_BIAS)\n",
    "# Track training progress\n",
    "global_train_losses = []\n",
    "total_train_loss = 0.0\n",
    "total_train_steps = 0\n",
    "print_interval = 10\n",
    "\n",
    "# Track best dev accuracy for model saving\n",
    "# This only works for epoch > 1 \n",
    "best_dev_acc = 0.0\n",
    "SAVE_DIR = \"best_model\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    model.train()\n",
    "    # Iterate over batches\n",
    "    loop = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in loop:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)        # [B, seq_len]\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch.get(\"labels\").to(DEVICE)                    # [B, seq_len]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hidden_states = model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']  # [B, seq_len, hidden]\n",
    "\n",
    "        \"\"\"\n",
    "        TODO-9: Compute next-token loss from hidden states and update model parameters.\n",
    "\n",
    "        Implementation hints:\n",
    "        1. Convert hidden states to logits over the vocabulary using model.hidden_state_to_token.\n",
    "        2. Shift logits and labels for next-token prediction to align each prediction with the correct next token.\n",
    "        3. Compute the cross-entropy loss, making sure positions with label=-100 are ignored.\n",
    "        4. Backpropagate and update model parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "        # Convert hidden states to logits over the vocabulary using model.hidden_state_to_token.\n",
    "        logits = model.hidden_state_to_token(hidden_states)\n",
    "\n",
    "        # Shift logits and labels for next-token prediction to align each prediction with the correct next token.\n",
    "        logits = logits[:, :-1, :]\n",
    "        labels = labels[:, 1:]\n",
    "\n",
    "        # Compute the cross-entropy loss, making sure positions with label=-100 are ignored.\n",
    "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), labels.reshape(-1), ignore_index=-100)\n",
    "\n",
    "        # Backpropagate and update the model parameters.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # raise NotImplementedError\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        total_train_steps += 1\n",
    "        global_train_avg_loss = total_train_loss / total_train_steps\n",
    "        global_train_losses.append(global_train_avg_loss)\n",
    "\n",
    "        loop.set_postfix({'avg_loss': f\"{global_train_avg_loss:.4f}\"})\n",
    "        \n",
    "    print(f\"Epoch {epoch+1} finished | Global Avg Loss: {global_train_avg_loss:.4f}\")\n",
    "\n",
    "    acc, all_preds, all_labels = evaluate_gpt2_xnli(model, tokenizer, dev_loader, max_gen_length=1, device=DEVICE)\n",
    "\n",
    "\n",
    "    if acc > best_dev_acc:\n",
    "        best_dev_acc = acc\n",
    "        torch.save(model.state_dict(), f\"{SAVE_DIR}/model.pt\")\n",
    "        print(f\"New best model saved at {SAVE_DIR}/model.pt with dev accuracy {best_dev_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3656cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|| 5010/5010 [01:01<00:00, 81.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 77.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: after fine-tuning, the accuracy should be better than random guessing (33.33%)\n",
    "# The accuracy we got is around 77.96% using whole training data and 1 epoch\n",
    "SAVE_DIR = \"best_model\"\n",
    "finetuned_model = GPT2Model(GPT2Config()).to(DEVICE)\n",
    "finetuned_model.load_state_dict(torch.load(f\"{SAVE_DIR}/model.pt\"))\n",
    "test_loader = DataLoader(test_dataset,shuffle=False,collate_fn=XNLIDataset.collate_fn)\n",
    "acc, all_preds, all_labels = evaluate_gpt2_xnli(finetuned_model, tokenizer, test_loader, max_gen_length=1, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354805ec",
   "metadata": {},
   "source": [
    "# Task 3: Multilingual NLI with GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0ee4ba",
   "metadata": {},
   "source": [
    "In this task, you will:\n",
    "\n",
    "- Test the fine-tuned GPT-2 on non-English languages for zero-shot cross-lingual transfer.\n",
    "\n",
    "- For each non-English language, fine-tune a model on the corresponding training set.\n",
    "\n",
    "- Fine-tune a unified model on the training sets of all languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd57fdf1",
   "metadata": {},
   "source": [
    "## Zero-shot Cross-lingual Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a247f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['en', 'ar', 'bg', 'de','el','es','fr','hi','ru','sw','th','tr','ur','vi','zh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f58b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized: split='test', lang='en', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='ar', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='bg', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='de', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='el', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='es', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='fr', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='hi', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='ru', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='sw', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='th', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='tr', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='ur', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='vi', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='zh', total=5010, subset=1, subset_count=5010\n",
      "Evaluating zero-shot on en...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|| 5010/5010 [00:58<00:00, 86.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 77.96%\n",
      "Evaluating zero-shot on ar...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|| 5010/5010 [01:06<00:00, 75.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 35.65%\n",
      "Evaluating zero-shot on bg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|| 5010/5010 [01:08<00:00, 73.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 35.15%\n",
      "Evaluating zero-shot on de...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|| 5010/5010 [01:00<00:00, 82.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 40.44%\n",
      "Evaluating zero-shot on el...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|| 5010/5010 [01:10<00:00, 71.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 35.05%\n",
      "Evaluating zero-shot on es...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|| 5010/5010 [01:00<00:00, 83.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 45.89%\n",
      "Evaluating zero-shot on fr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|| 5010/5010 [01:00<00:00, 82.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 41.32%\n",
      "Evaluating zero-shot on hi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|| 5010/5010 [01:13<00:00, 68.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 35.87%\n",
      "Evaluating zero-shot on ru...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|| 5010/5010 [01:08<00:00, 72.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 35.17%\n",
      "Evaluating zero-shot on sw...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|| 5010/5010 [01:01<00:00, 80.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 38.66%\n",
      "Evaluating zero-shot on th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|| 5010/5010 [01:17<00:00, 64.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 34.23%\n",
      "Evaluating zero-shot on tr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|| 5010/5010 [01:00<00:00, 82.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 39.86%\n",
      "Evaluating zero-shot on ur...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|| 5010/5010 [01:09<00:00, 72.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 34.07%\n",
      "Evaluating zero-shot on vi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|| 5010/5010 [01:06<00:00, 75.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 36.71%\n",
      "Evaluating zero-shot on zh...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|| 5010/5010 [01:05<00:00, 75.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 35.45%\n",
      "Zero-shot cross-lingual accuracy per language:\n",
      "en: 77.96%\n",
      "ar: 35.65%\n",
      "bg: 35.15%\n",
      "de: 40.44%\n",
      "el: 35.05%\n",
      "es: 45.89%\n",
      "fr: 41.32%\n",
      "hi: 35.87%\n",
      "ru: 35.17%\n",
      "sw: 38.66%\n",
      "th: 34.23%\n",
      "tr: 39.86%\n",
      "ur: 34.07%\n",
      "vi: 36.71%\n",
      "zh: 35.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_SUBSET = 1\n",
    "finetuned_model = GPT2Model(GPT2Config()).to(DEVICE)\n",
    "finetuned_model.load_state_dict(torch.load(path))\n",
    "all_test_datasets = {}\n",
    "all_test_loader = {}\n",
    "for lang in langs:\n",
    "    test_dataset = XNLIDataset(split=\"test\", lang=lang, tokenizer=tokenizer, max_length=1024, subset=TEST_SUBSET)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=XNLIDataset.collate_fn)\n",
    "    all_test_datasets[lang] = test_dataset\n",
    "    all_test_loader[lang] = test_loader\n",
    "\n",
    "all_results = {}\n",
    "for lang in langs:\n",
    "    test_loader = all_test_loader[lang]\n",
    "    if lang == \"en\":\n",
    "        print(f\"Evaluating on {lang}...\")\n",
    "    else:\n",
    "        print(f\"Evaluating zero-shot on {lang}...\")\n",
    "    acc, all_preds, all_labels = evaluate_gpt2_xnli(finetuned_model, tokenizer, test_loader, max_gen_length=1, device=DEVICE)\n",
    "    all_results[lang] = acc\n",
    "\n",
    "print(\"Zero-shot cross-lingual accuracy per language:\")\n",
    "for lang, acc in all_results.items():\n",
    "    print(f\"{lang}: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491d87e2",
   "metadata": {},
   "source": [
    "## Fertility-based Language Selection\n",
    "\n",
    "Guidance: You may notice that some languages achieve reasonable zero-shot cross-lingual performance. This is likely because these languages are closer to English (e.g., in writing system), making cross-lingual transfer from English easier. However, many other languages perform close to random guessing, which is expected since GPT-2 was pretrained entirely on English data.\n",
    "\n",
    "To perform further multilingual fine-tuning, we need to identify which languages GPT-2 can realistically support (because if a language is not supported, fine-tuning on it will have little effect). A straightforward way to check this is to inspect the tokens in the models tokenizer. However, this is not practical for GPT-2-like models, because they use a Byte-Pair Encoding (BPE) tokenizer. BPE can decompose any Unicode string into subwords, even if the string never appeared in training, making it difficult to determine whether a language is truly supported.\n",
    "\n",
    "Instead, we can approximate tokenizer support using fertility, a metric that measures the average number of subwords produced per word. Lower fertility indicates better tokenizer quality and compression, while high fertility suggests heavy fragmentation, which can hurt model performance. By combining fertility analysis with zero-shot cross-lingual results, we can identify a subset of languages that GPT-2 can reasonably handle (a rough estimate, as officially GPT-2 is designed for English). Then, we can proceed with multilingual fine-tuning experiments on these languages.\n",
    "\n",
    "Reference: How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28acaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fertility(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Compute average fertility for a dataset.\n",
    "    Fertility = #tokens / #words\n",
    "    Note: word splitting is approximate and uses whitespace.\n",
    "    \"\"\"\n",
    "    total_words = 0\n",
    "    total_tokens = 0\n",
    "    samples = len(dataset)\n",
    "    \n",
    "    for i in tqdm(range(samples), desc=\"Computing fertility\"):\n",
    "        row = dataset.data.iloc[i]\n",
    "        for sent in [row['premise'], row['hypo']]:\n",
    "            words = sent.strip().split()  # crude word estimate\n",
    "            tokens = tokenizer.tokenize(sent)\n",
    "            total_words += len(words)\n",
    "            total_tokens += len(tokens)\n",
    "    \n",
    "    fertility = total_tokens / total_words if total_words > 0 else 0.0\n",
    "    return fertility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce5b291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized: split='train', lang='en', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|| 3927/3927 [00:00<00:00, 5710.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en: fertility = 1.11\n",
      "Dataset initialized: split='train', lang='ar', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|| 3927/3927 [00:00<00:00, 4382.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar: fertility = 4.70\n",
      "Dataset initialized: split='train', lang='bg', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|| 3927/3927 [00:01<00:00, 3854.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bg: fertility = 5.53\n",
      "Dataset initialized: split='train', lang='de', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|| 3927/3927 [00:00<00:00, 5106.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de: fertility = 2.10\n",
      "Dataset initialized: split='train', lang='el', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|| 3927/3927 [00:01<00:00, 3838.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el: fertility = 6.16\n",
      "Dataset initialized: split='train', lang='es', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|| 3927/3927 [00:00<00:00, 5528.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es: fertility = 1.83\n",
      "Dataset initialized: split='train', lang='fr', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|| 3927/3927 [00:00<00:00, 5423.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr: fertility = 1.75\n",
      "Dataset initialized: split='train', lang='hi', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|| 3927/3927 [00:00<00:00, 4503.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi: fertility = 5.12\n",
      "Dataset initialized: split='train', lang='ru', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|| 3927/3927 [00:01<00:00, 3575.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ru: fertility = 5.90\n",
      "Dataset initialized: split='train', lang='sw', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|| 3927/3927 [00:00<00:00, 7045.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sw: fertility = 2.08\n",
      "Dataset initialized: split='train', lang='th', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|| 3927/3927 [00:00<00:00, 4486.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th: fertility = 9.48\n",
      "Dataset initialized: split='train', lang='tr', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|| 3927/3927 [00:00<00:00, 5577.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr: fertility = 2.73\n",
      "Dataset initialized: split='train', lang='ur', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|| 3927/3927 [00:00<00:00, 6555.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ur: fertility = 5.16\n",
      "Dataset initialized: split='train', lang='vi', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|| 3927/3927 [00:00<00:00, 6479.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vi: fertility = 3.62\n",
      "Dataset initialized: split='train', lang='zh', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|| 3927/3927 [00:00<00:00, 6729.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zh: fertility = 3.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "subset_for_check = 0.01\n",
    "\n",
    "for lang in langs:\n",
    "    train_dataset = XNLIDataset(\n",
    "        split=\"train\",\n",
    "        lang=lang,\n",
    "        tokenizer=tokenizer,\n",
    "        subset=subset_for_check\n",
    "    )\n",
    "    fertility_score = compute_fertility(train_dataset, tokenizer)\n",
    "    print(f\"{lang}: fertility = {fertility_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37090f47",
   "metadata": {},
   "source": [
    "## Fine-tune GPT-2 (per-language)\n",
    "\n",
    "Guidance: Load the pretrained GPT-2 (not the ones fine-tuned on English NLI) along with the training data for a single target language. Choose non-English languages that performed well in the zero-shot cross-lingual transfer and fertility evaluation. It depends on you how many languages to include. Fine-tune a separate model for each selected language. Afterwards, compare these per-language fine-tuned models with the zero-shot cross-lingual transfer results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb1bee",
   "metadata": {},
   "source": [
    "## Fine-tune GPT-2 (all)\n",
    "\n",
    "Guidance: Load the pretrained GPT-2 (again, not the ones fine-tuned on English NLI) along with the training data for all target languages, including English. For non-English languages, select those that performed well in the zero-shot cross-lingual transfer and fertility evaluation. It depends on you how many languages to include. Fine-tune a single model on this combined multilingual dataset. Afterwards, compare this model with the per-language fine-tuned models and the zero-shot cross-lingual transfer results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
